{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Workflow\n",
    "\n",
    "This notebook demonstrates the data preparation workflow for the Snow Drought Index package. It covers loading data, preprocessing, station extraction and filtering, and data availability assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install missing packages\n",
    "#%pip install bottleneck\n",
    "#%pip install git+https://github.com/Nadiesenali/snowdroughtindex-main   # Install snowdroughtindex from the local source directory\n",
    "#%pip install --force-reinstall git+https://github.com/Nadiesenali/snowdroughtindex-main\n",
    "\n",
    "# Import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from shapely.geometry import Point\n",
    "import sys\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from scipy.interpolate import interp1d\n",
    "import random\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "\n",
    "# Import snowdroughtindex package\n",
    "from snowdroughtindex.core import data_preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll load the SWE data and other required datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "canswe_path = project_root / 'data' / 'input_data' /'CanSWE'/'CanSWE-CanEEN_1928-2024_v7.nc'\n",
    "CaSR_path = project_root / 'data' / 'output_data' /'casr_data'/'bow_combined_data.csv'\n",
    "basin_path = project_root / 'data' / 'input_data' /'Elevation'/'Bow_elevation_combined.shp'\n",
    "output_data = project_root / 'data' /'output_data'/'FROSTBYTE//'\n",
    "output_plots = project_root / 'data' /'output_plots'/'FROSTBYTE//'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using the implemented functions\n",
    "canswe = data_preparation.load_swe_data(canswe_path)\n",
    "casr_data = pd.read_csv(CaSR_path)\n",
    "bow_basin = data_preparation.load_basin_data(basin_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract CanSWE data for stations within the Bow basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get station locations from canswe\n",
    "stations_df = pd.DataFrame({\n",
    "\t'station_id': canswe['station_id'].values,\n",
    "\t'lat': canswe['lat'].values,\n",
    "\t'lon': canswe['lon'].values\n",
    "})\n",
    "\n",
    "# Create Point geometries for each station\n",
    "stations_gdf = gpd.GeoDataFrame(\n",
    "\tstations_df,\n",
    "\tgeometry=gpd.points_from_xy(stations_df['lon'], stations_df['lat']),\n",
    "\tcrs=bow_basin.crs\n",
    ")\n",
    "\n",
    "# Find stations within any of the Bow basin polygons\n",
    "stations_in_basin = stations_gdf[stations_gdf.within(bow_basin.unary_union)]\n",
    "\n",
    "# Select these stations from canswe\n",
    "bow_canswe = canswe.sel(station_id=stations_in_basin['station_id'].values)\n",
    "\n",
    "# Convert to DataFrame\n",
    "#bow_canswe_df = bow_canswe.to_dataframe().reset_index()\n",
    "\n",
    "# Save the extracted data to a nc file\n",
    "#bow_canswe.to_netcdf(output_data + 'bow_canswe.nc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract CaSRv3.1 SWE data within the Bow basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CaSR dataframe\n",
    "casr_df = pd.DataFrame(casr_data)\n",
    "\n",
    "# kee SWE, 'date', 'Grid_id', 'Elevation_category\n",
    "casr_df = casr_df[['time', 'Grid_id','lat', 'lon', 'Elevation_Category','SWE']]\n",
    "\n",
    "display(casr_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to geodataframe\n",
    "casr_gdf = gpd.GeoDataFrame(\n",
    "    casr_df,\n",
    "    geometry=gpd.points_from_xy(casr_df['lon'], casr_df['lat']),\n",
    "    crs=bow_basin.crs\n",
    ")\n",
    "display(casr_gdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Bow basin polygons colored by elevation category\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Create elevation class column based on 'mean' elevation\n",
    "bins = [500, 1000, 1500, 2000, 2500]\n",
    "labels = ['500_1000m', '1000_1500m', '1500_2000m', '2000_2500m']\n",
    "bow_basin['elev_class'] = pd.cut(bow_basin['mean'], bins=bins, labels=labels, include_lowest=True, right=False)\n",
    "\n",
    "# Ensure elev_class is a categorical type with desired order\n",
    "bow_basin['elev_class'] = pd.Categorical(\n",
    "    bow_basin['elev_class'],\n",
    "    categories=labels,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "bow_basin.plot(ax=ax, column='elev_class', cmap='PuBu', legend=True, edgecolor='black')\n",
    "# Add station points\n",
    "stations_in_basin.plot(ax=ax, color='red', markersize=20, label='CanSWE Stations')\n",
    "casr_gdf.plot(ax=ax, color='orange', markersize=5, label='CaSR grid points')\n",
    "ax.set_title('CanSWE data stations')\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.legend()\n",
    "plt.savefig(str(output_plots / 'bow_basin_by_elev_class.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FROSTBYTE gap filling\n",
    "\n",
    "FROSTBYTE workflow  (In the original workflow, SCDNA precipitation data were also used to increase the data availability. However, here I'm only using CanSWE data because I couldn't find compatible precipitation dataset for the time period 1980-2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set user-specified variables\n",
    "flag_buffer_default, buffer_km_default = 0, 0 # buffer flag (0: no buffer around test basin, 1: buffer of value buffer_default around test basin) and buffer default value in km to be applied if flag = 1\n",
    "month_start_water_year_default, day_start_water_year_default = 10, 1  # water year start\n",
    "month_end_water_year_default, day_end_water_year_default = 9, 30  # water year end\n",
    "min_obs_corr_default = 3 # the minimum number of overlapping observations required to calculate the correlation between 2 stations\n",
    "min_obs_cdf_default = 10 # the minimum number of observations required to calculate a station's cdf\n",
    "min_corr_default = 0.6 # the minimum correlation value required for donor stations to be selected\n",
    "window_days_default = 7 # the number of days used on either side of the infilling date for gap filling calculations\n",
    "min_obs_KGE_default = 3 # the minimum number of observations required to calculate the KGE''\n",
    "max_gap_days_default = 15  # max. number of days for gaps allowed in the daily SWE data for the linear interpolation\n",
    "artificial_gap_perc_default = 100 # the percentage of observations to remove during the artificial gap filling for each station & month's first day\n",
    "iterations_default = 1 # the number of times we repeat the artificial gap filling\n",
    "artificial_gap_filling_flag = 0 # indicates whether artificial gap filling is performed (1) or not (0)\n",
    "artificial_gap_filling_basins = ['all'] # a list of the basin(s) to run the gap filling for. To include all basins simply write 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def artificial_gap_filling(original_data, iterations, artificial_gap_perc, window_days, min_obs_corr, min_obs_cdf, min_corr, min_obs_KGE, flag):\n",
    "\n",
    "    \"\"\"Creating random artificial gaps in the original dataset for each month & station, and running the gap filling function to assess its performance.\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------------------\n",
    "    - original_data: Pandas DataFrame of original stations' observations dataset, to which data will be removed for artificial gap filling\n",
    "    - iterations: Positive integer denoting the number of times we want to repeat the artificial gap filling (we remove data at random each time in the original dataset)\n",
    "    - artificial_gap_perc: Percentage between 1 and 100 for the amount of data to remove for each station & month's first day\n",
    "    - window_days: Positive integer denoting the number of days to select data for around a certain doy, to calculate correlations\n",
    "    - min_obs_corr: Positive integer for the minimum number of overlapping observations required to calculate the correlation between 2 stations\n",
    "    - min_obs_cdf: Positive integer for the minimum number of stations required to calculate a station's cdf\n",
    "    - min_corr: Value between 0 and 1 for the minimum correlation value required to keep a donor station\n",
    "    - min_obs_KGE: Positive integer for the minimum number of stations required to calculate a station's cdf\n",
    "    - flag: Integer to plot the gap filled values vs the observed values (1) or not (0)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    - evaluation: Dictionary containing the artificial gap filling evaluation results for several metrics for each month's first day, station & iteration\n",
    "    - fig (optional): A figure of the gap filled vs. the actual SWE observations for each first day of the month\n",
    "\n",
    "    \"\"\"\n",
    "    # suppresses the \"SettingWithCopyWarning\"\n",
    "    pd.set_option(\"mode.chained_assignment\", None)\n",
    "\n",
    "    # Set up the figure\n",
    "    if flag == 1:\n",
    "        ncols = 3\n",
    "        fig, axs = plt.subplots(4, ncols, sharex=False, sharey=False, figsize=(8,10))\n",
    "        plot_col = -1\n",
    "        row = 0\n",
    "\n",
    "    # Identify stations for gap filling (without P & external SWE stations (buffer) as we don't do any gap filling for these)\n",
    "    cols = [c for c in original_data.columns if 'precip' not in c and 'ext' not in c]\n",
    "\n",
    "    # Create an empty dictionary to store the metric values for each month, station & iteration\n",
    "    evaluation = {}\n",
    "    metrics = ['RMSE', \"KGE''\", \"KGE''_corr\", \"KGE''_bias\", \"KGE''_var\"]\n",
    "    for m in metrics:\n",
    "        evaluation[m] = np.ones((12, len(cols), iterations)) * np.nan\n",
    "\n",
    "    # Calculate correlations between stations that have overlapping observations\n",
    "    corr = calculate_stations_doy_corr(original_data, window_days, min_obs_corr)\n",
    "\n",
    "    # loop over months\n",
    "    for mo in range(1,12+1):\n",
    "\n",
    "        # controls for plotting on right subplot\n",
    "        if flag == 1:\n",
    "            plot_col += 1\n",
    "            if plot_col == ncols:\n",
    "                row += 1\n",
    "                plot_col = 0\n",
    "\n",
    "        # loop over iterations\n",
    "        for i in range(iterations):\n",
    "\n",
    "            # initialize counter to assign results to the right station\n",
    "            elem = -1\n",
    "\n",
    "            # looping over stations\n",
    "            for s in cols:\n",
    "\n",
    "                # update counter to assign results to the right station\n",
    "                elem += 1\n",
    "\n",
    "                # duplicate original data to create artificial gaps from this\n",
    "                artificial_gaps_data = original_data.copy()\n",
    "\n",
    "                # remove all missing values for a given station for which to perform gap filling\n",
    "                station_nomissing_values = pd.DataFrame(artificial_gaps_data[s].dropna())\n",
    "\n",
    "                # add DOY to select data to gap fill within a time window around first day of month\n",
    "                station_nomissing_values['doy'] = station_nomissing_values.index.dayofyear\n",
    "\n",
    "                # calculate the doy corresponding to the date - using 2010 as common year (not leap year)\n",
    "                doy = int(datetime.datetime(2010,mo,1).strftime('%j'))\n",
    "\n",
    "                # calculate the start & end doys of the time window for quantile mapping, with caution around the start & end of the calendar year\n",
    "                window_startdoy = (doy-window_days)%365\n",
    "                window_startdoy = 365 if window_startdoy == 0 else window_startdoy\n",
    "                window_enddoy = (doy+window_days)%365\n",
    "                window_enddoy = 366 if window_enddoy == 0 or window_enddoy == 365 else window_enddoy\n",
    "\n",
    "                # select data within time window\n",
    "                if window_startdoy > window_enddoy:\n",
    "                    data_window = station_nomissing_values[(station_nomissing_values['doy']>=window_startdoy) | (station_nomissing_values['doy'] <= window_enddoy)]\n",
    "                else:\n",
    "                    data_window = station_nomissing_values[(station_nomissing_values['doy']>=window_startdoy) & (station_nomissing_values['doy'] <= window_enddoy)]\n",
    "\n",
    "                # Select target data within this time window\n",
    "                data_window_target = data_window[s]\n",
    "\n",
    "                # calculate the number of observations to remove for this station & month's first day\n",
    "                n = int(len(data_window.index) * artificial_gap_perc / 100)\n",
    "\n",
    "                # if the number of observations is above zero we can proceed with the gap filling\n",
    "                if n > 0:\n",
    "\n",
    "                    # randomly select n dates from the station's data (no duplicates) and remove them from the original dataset - if 100% is removed then all dates will be selected\n",
    "                    if artificial_gap_perc == 100:\n",
    "                        dates_to_remove = data_window.index\n",
    "                    else:\n",
    "                        dates_to_remove = data_window.index[random.sample(range(0, len(data_window.index)), n)]\n",
    "                    artificial_gaps_data[s].loc[dates_to_remove] = np.nan\n",
    "                    artificial_gaps_data = artificial_gaps_data.loc[dates_to_remove]\n",
    "\n",
    "                    # Keep only SWE station to gap fill\n",
    "                    gapfilled_data = artificial_gaps_data[s].copy()\n",
    "\n",
    "                    # Identify dates for gap filling\n",
    "                    time_index = data_window.dropna().index\n",
    "\n",
    "                    # Loop over dates for gap filling\n",
    "                    for d in time_index:\n",
    "\n",
    "                        # Get the doy corresponding to the date\n",
    "                        doy = data_window.dropna().loc[d,'doy']\n",
    "\n",
    "                        # Get IDs of all stations with data for this date (and within time window)\n",
    "                        data_window_allstations = artificial_gaps_data.dropna(axis=1, how='all')\n",
    "                        non_missing_stations = [c for c in data_window_allstations.columns]\n",
    "                        data_window_allstations['days_to_date'] = abs((d - data_window_allstations.index).days)\n",
    "\n",
    "                        # We can continue if there are enough target data to build cdf\n",
    "                        if len(data_window_target.index) >= min_obs_cdf:\n",
    "\n",
    "                            # Get ids of all stations with correlations >= a minimum correlation for this doy, not including the target station itself\n",
    "                            non_missing_corr = corr[doy][s].dropna()\n",
    "                            non_missing_corr = non_missing_corr[non_missing_corr.index.isin(non_missing_stations)]\n",
    "                            potential_donor_stations = non_missing_corr[non_missing_corr >= min_corr].index.values\n",
    "                            potential_donor_stations = [c for c in potential_donor_stations if s not in c]\n",
    "\n",
    "                            # If there is at least one potential donor station, proceed\n",
    "                            if len(potential_donor_stations) > 0:\n",
    "\n",
    "                                # Sort the donor stations from highest to lowest value\n",
    "                                potential_donor_stations_sorted = corr[doy].loc[potential_donor_stations,s].dropna().sort_values(ascending=False).index.values\n",
    "\n",
    "                                # Loop over sorted donor stations until I find one with enough data to build a cdf\n",
    "                                for donor_station in potential_donor_stations_sorted:\n",
    "\n",
    "                                    # Select data within time window for this doy from all years\n",
    "                                    data_window_donor = data_window_allstations[donor_station].dropna()\n",
    "\n",
    "                                    # We can continue if there are enough donor data to build cdf\n",
    "                                    if len(data_window_donor.index) >= min_obs_cdf:\n",
    "\n",
    "                                        # If the donor station has multiple values within the window, we keep the closest donor station value to the date we are gap filling\n",
    "                                        sorted_data_window = data_window_allstations.sort_values(by=['days_to_date'])\n",
    "                                        value_donor = sorted_data_window[donor_station].dropna()[0]\n",
    "\n",
    "                                        # Perform the gap filling using quantile mapping\n",
    "                                        value_target = quantile_mapping(data_window_donor, data_window_target, value_donor, min_obs_cdf, flag=0)\n",
    "\n",
    "                                        if value_target != None:\n",
    "                                            gapfilled_data.loc[d] = value_target\n",
    "\n",
    "                                        break\n",
    "\n",
    "                                    else:\n",
    "                                        continue\n",
    "\n",
    "                    # combine observed & predicted data into a single Pandas dataframe\n",
    "                    # results = gapfilled_data[s].loc[dates_to_remove]\n",
    "                    results = gapfilled_data.to_frame(name='pre')\n",
    "                    results['obs'] = original_data[s].loc[dates_to_remove]\n",
    "                    results = results.dropna()\n",
    "\n",
    "                    # plot the gap filled vs the observed values\n",
    "                    if flag == 1:\n",
    "                        axs[row,plot_col].scatter(results['obs'], results['pre'], color='b', alpha=.3)\n",
    "                        axs[row,plot_col].set_title('month'+str(mo))\n",
    "                        if row == 3 and plot_col == 0:\n",
    "                            axs[row,plot_col].set_xlabel('observed')\n",
    "                            axs[row,plot_col].set_ylabel('infilling')\n",
    "\n",
    "                    # if there are no predicted values set the metrics to nan\n",
    "                    if results.empty == True:\n",
    "                        for m in metrics:\n",
    "                            evaluation[m][mo-1,elem,i] = np.nan\n",
    "\n",
    "                    # otherwise proceed with evaluating the gap filling performance\n",
    "                    else:\n",
    "                        rmse = mean_squared_error(results['obs'], results['pre'], squared=False)\n",
    "                        kge_prime_prime = KGE_Tang2021(results['obs'].values, results['pre'].values, min_obs_KGE)\n",
    "                        evaluation['RMSE'][mo-1,elem,i] = rmse\n",
    "                        evaluation[\"KGE''\"][mo-1,elem,i] = kge_prime_prime['KGE']\n",
    "                        evaluation[\"KGE''_corr\"][mo-1,elem,i] = kge_prime_prime['r']\n",
    "                        evaluation[\"KGE''_bias\"][mo-1,elem,i] = kge_prime_prime['beta']\n",
    "                        evaluation[\"KGE''_var\"][mo-1,elem,i] = kge_prime_prime['alpha']\n",
    "\n",
    "                # else if the number of observations is zero we go to the next station\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "    if flag == 1:\n",
    "        plt.tight_layout()\n",
    "        return evaluation, fig\n",
    "\n",
    "    else:\n",
    "        return evaluation\n",
    "\n",
    "###\n",
    "\n",
    "def KGE_Tang2021(obs, pre, min_obs_KGE):\n",
    "\n",
    "    \"\"\"Calculates the modified Kling-Gupta Efficiency (KGE\") and its 3 components.\n",
    "    The KGE measures the correlation, bias and variability of the simulated values against the observed values.\n",
    "    KGE\" was proposed by Tang et al. (2021) to solve issues arising with 0 values in the KGE or KGE'.\n",
    "    For more info, see https://doi.org/10.1175/jcli-d-21-0067.1\n",
    "    KGE\" range: -Inf to 1. Perfect score: 1. Units: Unitless.\n",
    "    Correlation (r): Perfect score is 1.\n",
    "    Bias ratio (beta): Perfect score is 0.\n",
    "    Variability ratio (alpha):  Perfect score is 1.\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------------------\n",
    "    - obs: Numpy Array of observations to evaluate\n",
    "    - pre: Numpy Array of predictions/simulations to evaluate\n",
    "    - min_obs_KGE: Positive integer for the minimum number of stations required to calculate a station's cdf\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    - KGEgroup: Dictionary containing the final KGE'' value as well as all elements of the KGE''\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ind_nan = np.isnan(obs) | np.isnan(pre)\n",
    "    obs = obs[~ind_nan]\n",
    "    pre = pre[~ind_nan]\n",
    "\n",
    "    if len(obs) >= min_obs_KGE:\n",
    "\n",
    "        pre_mean = np.mean(pre, axis=0, dtype=np.float64)\n",
    "        obs_mean = np.mean(obs, axis=0, dtype=np.float64)\n",
    "        pre_std = np.std(pre, axis=0)\n",
    "        obs_std = np.std(obs, dtype=np.float64)\n",
    "\n",
    "        # Check to see if all forecast values are the same. If they are r cannot be calculated and is set to 0\n",
    "        # For more info: https://doi.org/10.5194/hess-23-4323-2019 (Section 2)\n",
    "        if pre_std == 0:\n",
    "\n",
    "            r = 0\n",
    "\n",
    "        else:\n",
    "\n",
    "            r = np.sum((pre - pre_mean) * (obs - obs_mean), axis=0, dtype=np.float64) / \\\n",
    "                np.sqrt(np.sum((pre - pre_mean) ** 2, axis=0, dtype=np.float64) *\n",
    "                        np.sum((obs - obs_mean) ** 2, dtype=np.float64))\n",
    "\n",
    "        alpha = pre_std / obs_std\n",
    "\n",
    "        beta = (pre_mean - obs_mean) / obs_std\n",
    "\n",
    "        KGE = 1 - np.sqrt((r - 1) ** 2 + (alpha - 1) ** 2 + (beta) ** 2)\n",
    "\n",
    "        KGEgroup = {'KGE': KGE, 'r': r, 'alpha': alpha, 'beta': beta}\n",
    "\n",
    "    else:\n",
    "\n",
    "        KGEgroup = {'KGE': np.nan, 'r': np.nan, 'alpha': np.nan, 'beta': np.nan}\n",
    "\n",
    "    return KGEgroup\n",
    "\n",
    "###\n",
    "def data_availability_monthly_plots_1(SWE_stations, original_SWE_data, gapfilled_SWE_data, flag):\n",
    "\n",
    "    \"\"\"Calculating and plotting the % of SWE stations available on the first day of each month of each year.\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------------------\n",
    "    - SWE_stations: Pandas GeoDataFrame of all SWE stations\n",
    "    - original_SWE_data: xarray DataArray of the original SWE observations\n",
    "    - gapfilled_SWE_data: xarray DataArray of the SWE observations after gap filling\n",
    "    - flag: Flag to indicate if gap filled data was provided (1) or not (0). In the case that it is provided, a comparison plot will be made to compare data availability in the original data vs the gap filled data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    - Bar chart timeseries of SWE stations available on the first day of each month of each year\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize plot\n",
    "    fig, axs = plt.subplots(6, 2, sharex=True, sharey=True, figsize=(14,8))\n",
    "    elem = -1\n",
    "    column = 0\n",
    "\n",
    "    # Loop over months\n",
    "    for m in range(1,12+1):\n",
    "\n",
    "        # controls for plotting on right subplot (i.e., month)\n",
    "        elem += 1\n",
    "        if elem == 6:\n",
    "            column += 1\n",
    "            elem = 0\n",
    "\n",
    "        # for SWE data with gap filling\n",
    "        if flag == 1:\n",
    "\n",
    "            # extract data on the first of the month m\n",
    "            data_month_gapfilled = gapfilled_SWE_data.sel(station_id=SWE_stations.station_id.values, time=( (gapfilled_SWE_data['time.month'] == m) & (gapfilled_SWE_data['time.day'] == 1) ))\n",
    "\n",
    "            # count the % of stations with data on those dates\n",
    "            data_month_gapfilled_count = data_month_gapfilled.count(dim='station_id') / len(SWE_stations) * 100\n",
    "\n",
    "            # plot bar chart of available data\n",
    "            axs[elem,column].bar(data_month_gapfilled_count['time.year'], data_month_gapfilled_count.data, color='r', alpha=.5)\n",
    "\n",
    "        # same process as above but for original SWE data\n",
    "        data_month = original_SWE_data.sel(station_id=SWE_stations.station_id.values, time=( (original_SWE_data['time.month'] == m) & (original_SWE_data['time.day'] == 1) ))\n",
    "        data_month_count = data_month.count(dim='station_id') / len(SWE_stations) * 100\n",
    "        axs[elem,column].bar(data_month_count['time.year'], data_month_count.data, color='b')\n",
    "\n",
    "        # add plot labels\n",
    "        if elem == 5 and column == 0:\n",
    "            axs[elem,column].set_ylabel('% of SWE stations \\n with data in basin')\n",
    "        month_name = datetime.datetime.strptime(str(m), \"%m\").strftime(\"%b\")\n",
    "        axs[elem,column].set_title('1st '+month_name, fontweight='bold')\n",
    "\n",
    "        if flag == 1:\n",
    "            bluepatch = mpatches.Patch(color='b', label='original data')\n",
    "            redpatch = mpatches.Patch(color='r', alpha=.5, label='after gap filling')\n",
    "            plt.legend(handles=[bluepatch, redpatch])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "###\n",
    "\n",
    "def data_availability_monthly_plots_2(SWE_data):\n",
    "\n",
    "    \"\"\"Creating bar chart subplots of the days with SWE observations around the 1st day of each month.\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------------------\n",
    "    - SWE_data: Pandas DataFrame containing the SWE stations observations\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    - Bar chart subplots of the days with SWE observations around the 1st day of each month\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize plot\n",
    "    fig, axs = plt.subplots(6, 2, sharex=False, sharey=True, figsize=(8,16))\n",
    "    elem = -1\n",
    "    column = 0\n",
    "\n",
    "    # Add day of year (doy) to test basin SWE observations Pandas DataFrame\n",
    "    SWE_data_with_doy = SWE_data.copy()\n",
    "    SWE_data_with_doy['doy'] = SWE_data_with_doy.index.dayofyear\n",
    "\n",
    "    # Remove automatic stations as they distract the analysis\n",
    "    manual_stations = [s for s in SWE_data_with_doy.columns if s[-1] != 'P']\n",
    "    SWE_data_with_doy_manual = SWE_data_with_doy[manual_stations]\n",
    "\n",
    "    # Define the doys of 1st of each month\n",
    "    doys_first_month = [1, 32, 60, 91, 121, 152, 182, 213, 244, 274, 305, 335]\n",
    "\n",
    "    # Loop over months\n",
    "    for m in range(1,12+1):\n",
    "\n",
    "        # controls for plotting on right subplot\n",
    "        elem += 1\n",
    "        if elem == 6:\n",
    "            column += 1\n",
    "            elem = 0\n",
    "\n",
    "        # calculate the start & end of the data selection window, with caution around the start & end of the calendar year\n",
    "        window_start = (doys_first_month[m-1]-15)%366\n",
    "        if window_start == 0:\n",
    "            window_start = 365\n",
    "        window_end = (doys_first_month[m-1]+15)%366\n",
    "        if window_end == 0 or window_end == 365:\n",
    "            window_end = 366\n",
    "\n",
    "        # select SWE observations within window\n",
    "        if window_start > window_end:\n",
    "            data_window = SWE_data_with_doy_manual[(SWE_data_with_doy_manual['doy']>=window_start) | (SWE_data_with_doy_manual['doy'] <= window_end)]\n",
    "        else:\n",
    "            data_window = SWE_data_with_doy_manual[(SWE_data_with_doy_manual['doy']>=window_start) & (SWE_data_with_doy_manual['doy'] <= window_end)]\n",
    "\n",
    "        # drop dates or stations with no data at all\n",
    "        data_window = data_window.dropna(axis=0, how='all')\n",
    "        data_window = data_window.dropna(axis=1, how='all')\n",
    "\n",
    "        # count total number of stations with data on each doy\n",
    "        stations_cols = [c for c in data_window.columns if 'doy' not in c]\n",
    "        data_stations_window = data_window[stations_cols]\n",
    "        data_count_window = data_stations_window.count(axis=1)\n",
    "\n",
    "        # create xticks to plot the data for each doy\n",
    "        if window_start > window_end:\n",
    "            xticks = list(np.arange(window_start,365+1))+list(np.arange(1,window_end+1))\n",
    "        else:\n",
    "            xticks = list(np.arange(window_start,window_end+1))\n",
    "        xticks_plot = np.arange(len(xticks))\n",
    "\n",
    "        # save the data for the right doy\n",
    "        data_count_plot = [0]*len(xticks)\n",
    "        for x in range(len(data_window.index)):\n",
    "            doy = data_window.iloc[x]['doy']\n",
    "            if doy == 366:\n",
    "                doy = 365\n",
    "            data_count_plot[xticks.index(doy)] += data_count_window.iloc[x]\n",
    "\n",
    "        # plot data\n",
    "        axs[elem,column].bar(xticks_plot, data_count_plot, color='b')\n",
    "        axs[elem,column].set_xticks([xticks_plot[0],xticks_plot[15],xticks_plot[-1]])\n",
    "        axs[elem,column].set_xticklabels([xticks[0],doys_first_month[m-1],xticks[-1]])\n",
    "\n",
    "        # add plot labels\n",
    "        if elem == 5 and column == 0:\n",
    "            axs[elem,column].set_ylabel('# of SWE obs.')\n",
    "            axs[elem,column].set_xlabel('DOY')\n",
    "\n",
    "        if elem == 5 and column == 1:\n",
    "            axs[elem,column].set_xlabel('DOY')\n",
    "\n",
    "        month_name = datetime.datetime.strptime(str(m), \"%m\").strftime(\"%b\")\n",
    "        axs[elem,column].set_title('1st '+month_name+' +/- 15 days', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "###\n",
    "def calculate_stations_doy_corr(stations_obs, window_days, min_obs_corr):\n",
    "\n",
    "    \"\"\"Calculating stations' correlations for each day of the year (doy; with a X-day window centered around the doy).\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------------------\n",
    "    - stations_obs: Pandas DataFrame of all SWE & P stations observations\n",
    "    - window_days: Positive integer denoting the number of days to select data for around a certain doy, to calculate correlations\n",
    "    - min_obs_corr: Positive integer for the minimum number of overlapping observations required to calculate the correlation between 2 stations\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    - stations_doy_corr: Dictionary containing a Pandas DataFrame of stations correlations for each day of year\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up the dictionary to save all correlations\n",
    "    stations_doy_corr = {}\n",
    "\n",
    "    # Duplicate the stations observations Pandas DataFrame and add doy column\n",
    "    stations_obs_doy = stations_obs.copy()\n",
    "    stations_obs_doy['doy'] = stations_obs_doy.index.dayofyear\n",
    "\n",
    "    # Loop over days of the year\n",
    "    for doy in range(1,366+1):\n",
    "\n",
    "        # calculate the start & end of the data selection window, with caution around the start & end of the calendar year\n",
    "        window_start = (doy-window_days)%366\n",
    "        window_start = 366 if window_start == 0 else window_start\n",
    "        window_end = (doy+window_days)%366\n",
    "        window_end = 366 if window_end == 0 else window_end\n",
    "\n",
    "        # select data for the window of interest\n",
    "        if window_start > window_end:\n",
    "            data_window = stations_obs_doy[(stations_obs_doy['doy']>=window_start) | (stations_obs_doy['doy'] <= window_end)]\n",
    "        else:\n",
    "            data_window = stations_obs_doy[(stations_obs_doy['doy']>=window_start) & (stations_obs_doy['doy'] <= window_end)]\n",
    "\n",
    "        # calculate the Pearson product-moment correlations between stations if the minimum number of observations criterium is met\n",
    "        data_window = data_window.drop(columns=['doy'])\n",
    "        corr = data_window.corr(method='spearman', min_periods=min_obs_corr)\n",
    "        # np.fill_diagonal(corr.values, np.nan)\n",
    "\n",
    "        # save correlation for the doy to the dictionary\n",
    "        stations_doy_corr[doy] = corr\n",
    "\n",
    "    return stations_doy_corr\n",
    "\n",
    "###\n",
    "\n",
    "def qm_gap_filling(original_data, window_days, min_obs_corr, min_obs_cdf, min_corr):\n",
    "\n",
    "    \"\"\"Performing the gap filling for all missing observations (when possible) using quantile mapping.\n",
    "    For each target station and each date for which date is missing, we identify a donor stations as the station with:\n",
    "    - data for this date,\n",
    "    - a cdf for this doy,\n",
    "    - and the best correlation to the target station (correlation >= min_corr for this doy).\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------------------\n",
    "    - original_data: Pandas DataFrame of original stations' observations dataset, which will be gap filled\n",
    "    - window_days: Positive integer denoting the number of days to select data for around a certain doy, to calculate correlations\n",
    "    - min_obs_corr: Positive integer for the minimum number of overlapping observations required to calculate the correlation between 2 stations\n",
    "    - min_obs_cdf: Positive integer for the minimum number of stations required to calculate a station's cdf\n",
    "    - min_corr: Value between 0 and 1 for the minimum correlation value required to keep a donor station\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    - gapfilled_data: Pandas DataFrame of gap filled stations' observations\n",
    "    - data_type_flags: Pandas DataFrame with information about the type of data (estimates or observations) in the gap filled dataset\n",
    "    - donor_stationIDs: Pandas DataFrame with information about the donor station used to fill each of the gaps\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a duplicate of the dataset to gap fill\n",
    "    gapfilled_data = original_data.copy()\n",
    "\n",
    "    # Remove P & external SWE stations (buffer) from the dataframe\n",
    "    cols = [c for c in original_data.columns if 'precip' not in c and 'ext' not in c]\n",
    "\n",
    "    # Keep only gap filled SWE stations (without P stations & external SWE stations)\n",
    "    gapfilled_data = gapfilled_data[cols]\n",
    "\n",
    "    # Add doy to the Pandas DataFrame\n",
    "    original_data['doy'] = original_data.index.dayofyear\n",
    "\n",
    "    # Set empty dataframes to keep track of data type and donor station ids\n",
    "    data_type_flags = pd.DataFrame(data=0, index=original_data.index, columns=cols)\n",
    "    donor_stationIDs = pd.DataFrame(data=np.nan, index=original_data.index, columns=cols)\n",
    "\n",
    "    # Calculate correlations between stations that have overlapping observations\n",
    "    corr = calculate_stations_doy_corr(original_data, window_days, min_obs_corr)\n",
    "\n",
    "    # Identify dates for gap filling\n",
    "    time_index = original_data.index\n",
    "\n",
    "    # Loop over dates for gap filling\n",
    "    for d in time_index:\n",
    "\n",
    "        # Calculate the doy corresponding to the date\n",
    "        # Note: doy 365 and 366 are bundled together\n",
    "        doy = original_data.loc[d,'doy']\n",
    "\n",
    "        # Calculate the start and end dates of the time window for the gap filling steps\n",
    "        window_startdate = d - pd.Timedelta(days=window_days)\n",
    "        window_enddate = d + pd.Timedelta(days=window_days)\n",
    "\n",
    "        # Get IDs of all stations with data for this date (and within time window)\n",
    "        data_window = original_data[window_startdate:window_enddate].dropna(axis=1, how='all')\n",
    "        non_missing_stations = [c for c in data_window.columns if 'doy' not in c]\n",
    "        data_window['days_to_date'] = abs((d - data_window.index).days)\n",
    "\n",
    "        # Calculate the start & end doys of the time window for quantile mapping, with special rules around the start & end of the calendar year\n",
    "        window_startdoy = (data_window['doy'].iloc[0])%366\n",
    "        window_startdoy = 366 if window_startdoy == 0 else window_startdoy\n",
    "        window_enddoy = (data_window['doy'].iloc[-1])%366\n",
    "        window_enddoy = 366 if window_enddoy == 0 else window_enddoy\n",
    "\n",
    "        # Loop over stations to gap fill\n",
    "        for target_station in cols:\n",
    "\n",
    "            # If station has no data, proceed with the gap filling\n",
    "            if np.isnan(original_data.loc[d,target_station]):\n",
    "\n",
    "                # Select target data within time window for this doy from all years\n",
    "                if window_startdoy > window_enddoy:\n",
    "                    data_window_target = original_data[target_station].dropna()[(original_data['doy']>=window_startdoy) | (original_data['doy'] <= window_enddoy)]\n",
    "                else:\n",
    "                    data_window_target = original_data[target_station].dropna()[(original_data['doy']>=window_startdoy) & (original_data['doy'] <= window_enddoy)]\n",
    "\n",
    "                # We can continue if there are enough target data to build cdf\n",
    "                if len(data_window_target.index) >= min_obs_cdf:\n",
    "\n",
    "                    # Get ids of all stations with correlations >= a minimum correlation for this doy, not including the target station itself\n",
    "                    non_missing_corr = corr[doy][target_station].dropna()\n",
    "                    non_missing_corr = non_missing_corr[non_missing_corr.index.isin(non_missing_stations)]\n",
    "                    potential_donor_stations = non_missing_corr[non_missing_corr >= min_corr].index.values\n",
    "                    potential_donor_stations = [c for c in potential_donor_stations if target_station not in c]\n",
    "\n",
    "                    # If there is at least one potential donor station, proceed\n",
    "                    if len(potential_donor_stations) > 0:\n",
    "\n",
    "                        # Sort the donor stations from highest to lowest value\n",
    "                        potential_donor_stations_sorted = corr[doy].loc[potential_donor_stations,target_station].dropna().sort_values(ascending=False).index.values\n",
    "\n",
    "                        # Loop over sorted donor stations until I find one with enough data to build a cdf\n",
    "                        for donor_station in potential_donor_stations_sorted:\n",
    "\n",
    "                            # Select data within time window for this doy from all years\n",
    "                            if window_startdoy > window_enddoy:\n",
    "                                data_window_donor = original_data[donor_station].dropna()[(original_data['doy'] >= window_startdoy) | (original_data['doy'] <= window_enddoy)]\n",
    "                            else:\n",
    "                                data_window_donor = original_data[donor_station].dropna()[(original_data['doy'] >= window_startdoy) & (original_data['doy'] <= window_enddoy)]\n",
    "\n",
    "                            # We can continue if there are enough donor data to build cdf\n",
    "                            if len(data_window_donor.index) >= min_obs_cdf:\n",
    "\n",
    "                                # If the donor station has multiple values within the window, we keep the closest donor station value to the date we are gap filling\n",
    "                                sorted_data_window = data_window.sort_values(by=['days_to_date'])\n",
    "                                value_donor = sorted_data_window[donor_station].dropna()[0]\n",
    "\n",
    "                                # Perform the gap filling using quantile mapping\n",
    "                                value_target = quantile_mapping(data_window_donor, data_window_target, value_donor, min_obs_cdf, flag=0)\n",
    "\n",
    "                                if value_target != None:\n",
    "                                    gapfilled_data.loc[d,target_station] = value_target\n",
    "                                    data_type_flags.loc[d,target_station] = 1\n",
    "                                    donor_stationIDs.loc[d,target_station] = donor_station\n",
    "\n",
    "                                break\n",
    "\n",
    "                            else:\n",
    "                                continue\n",
    "\n",
    "    return gapfilled_data, data_type_flags, donor_stationIDs\n",
    "\n",
    "###\n",
    "\n",
    "def quantile_mapping(data_donor, data_target, value_donor, min_obs_cdf, flag):\n",
    "\n",
    "    \"\"\"Calculating target station's gap filling value from donor station's value using quantile mapping.\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------------------\n",
    "    - data_donor: Pandas DataFrame of donor station observations used to build empirical cdf\n",
    "    - data_target: Pandas DataFrame of target station observations used to build empirical cdf\n",
    "    - value_donor: Integer of donor station value used in the quantile mapping\n",
    "    - min_obs_cdf: Positive integer for the minimum number of unique observations required to calculate a station's cdf\n",
    "    - flag: Integer to plot (1) or not (0) the donor and target stations' cdfs\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    - value_target: Integer of target station value calculated using quantile mapping\n",
    "    - plot of the donor and target stations' cdfs (optional)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # build the donor station's empirical cdf\n",
    "    sorted_data_donor = data_donor.drop_duplicates().sort_values(ignore_index=True)\n",
    "\n",
    "    # build the target station's empiral cdf\n",
    "    sorted_data_target = data_target.drop_duplicates().sort_values(ignore_index=True)\n",
    "\n",
    "    # Calculate the donor & target stations' cdfs if they both have at least X unique observations\n",
    "    if (len(sorted_data_donor) >= min_obs_cdf) & (len(sorted_data_target) >= min_obs_cdf):\n",
    "\n",
    "        # Calculate the cumulative probability corresponding to the donor value\n",
    "        rank_donor_obs = sorted_data_donor[sorted_data_donor == value_donor].index[0]\n",
    "        total_obs_donor = len(sorted_data_donor)\n",
    "        cumul_prob_donor_obs = (rank_donor_obs + 1) / total_obs_donor\n",
    "\n",
    "        # Calculate the cumulative probability corresponding to the target value\n",
    "        cumul_prob_target = np.arange(1,len(sorted_data_target)+1) / (len(sorted_data_target))\n",
    "\n",
    "        # inter-/extrapolate linearly to get the target value corresponding to the donor station's cumulative probability\n",
    "        inverted_edf = interp1d(cumul_prob_target, sorted_data_target, fill_value=\"extrapolate\")\n",
    "        value_target = round(float(inverted_edf(cumul_prob_donor_obs)),2)\n",
    "\n",
    "        # set any potential negative values from interpolation/extrapolation to zero\n",
    "        if(value_target) < 0:\n",
    "            value_target = 0\n",
    "\n",
    "        # if requested, plot the target & donor stations' cdfs\n",
    "        if flag == 1:\n",
    "            plt.figure()\n",
    "            plt.plot(sorted_data_donor, np.arange(1,len(sorted_data_donor)+1) / (len(sorted_data_donor)), label='donor')\n",
    "            plt.plot(sorted_data_target, cumul_prob_target, label='target')\n",
    "            plt.scatter(value_donor, cumul_prob_donor_obs)\n",
    "            plt.legend()\n",
    "\n",
    "        return value_target\n",
    "\n",
    "    # If either/both the target & donor stations have < X observations do nothing\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "###\n",
    "\n",
    "def quantile_mapping(data_donor, data_target, value_donor, min_obs_cdf, flag):\n",
    "\n",
    "    \"\"\"Calculating target station's gap filling value from donor station's value using quantile mapping.\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------------------\n",
    "    - data_donor: Pandas DataFrame of donor station observations used to build empirical cdf\n",
    "    - data_target: Pandas DataFrame of target station observations used to build empirical cdf\n",
    "    - value_donor: Integer of donor station value used in the quantile mapping\n",
    "    - min_obs_cdf: Positive integer for the minimum number of unique observations required to calculate a station's cdf\n",
    "    - flag: Integer to plot (1) or not (0) the donor and target stations' cdfs\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    - value_target: Integer of target station value calculated using quantile mapping\n",
    "    - plot of the donor and target stations' cdfs (optional)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # build the donor station's empirical cdf\n",
    "    sorted_data_donor = data_donor.drop_duplicates().sort_values(ignore_index=True)\n",
    "\n",
    "    # build the target station's empiral cdf\n",
    "    sorted_data_target = data_target.drop_duplicates().sort_values(ignore_index=True)\n",
    "\n",
    "    # Calculate the donor & target stations' cdfs if they both have at least X unique observations\n",
    "    if (len(sorted_data_donor) >= min_obs_cdf) & (len(sorted_data_target) >= min_obs_cdf):\n",
    "\n",
    "        # Calculate the cumulative probability corresponding to the donor value\n",
    "        rank_donor_obs = sorted_data_donor.reset_index(drop=True)[sorted_data_donor.values == value_donor].index[0]\n",
    "        total_obs_donor = len(sorted_data_donor)\n",
    "        cumul_prob_donor_obs = (rank_donor_obs + 1) / total_obs_donor\n",
    "\n",
    "        # Calculate the cumulative probability corresponding to the target value\n",
    "        cumul_prob_target = np.arange(1,len(sorted_data_target)+1) / (len(sorted_data_target))\n",
    "\n",
    "        # inter-/extrapolate linearly to get the target value corresponding to the donor station's cumulative probability\n",
    "        inverted_edf = interp1d(cumul_prob_target, sorted_data_target, fill_value=\"extrapolate\")\n",
    "        value_target = round(float(inverted_edf(cumul_prob_donor_obs)),2)\n",
    "\n",
    "        # set any potential negative values from interpolation/extrapolation to zero\n",
    "        if(value_target) < 0:\n",
    "            value_target = 0\n",
    "\n",
    "        # if requested, plot the target & donor stations' cdfs\n",
    "        if flag == 1:\n",
    "            plt.figure()\n",
    "            plt.plot(sorted_data_donor, np.arange(1,len(sorted_data_donor)+1) / (len(sorted_data_donor)), label='donor')\n",
    "            plt.plot(sorted_data_target, cumul_prob_target, label='target')\n",
    "            plt.scatter(value_donor, cumul_prob_donor_obs)\n",
    "            plt.legend()\n",
    "\n",
    "        return value_target\n",
    "\n",
    "    # If either/both the target & donor stations have < X observations do nothing\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open bow_canswe.csv file\n",
    "bow_canswe = pd.read_csv(output_data / 'bow_canswe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-organize the dataset as needed\n",
    "# Restrict bow_canswe to 1980-01-01 through 2024-07-31\n",
    "bow_canswe['time'] = pd.to_datetime(bow_canswe['time'])\n",
    "SWE_stations_df = bow_canswe[(bow_canswe['time'] >= '1980-01-01') & (bow_canswe['time'] <= '2023-07-31')]\n",
    "\n",
    "# If you want to convert this DataFrame to an xarray Dataset similar to the original code:\n",
    "SWE_stations_ds = SWE_stations_df.set_index(['station_id', 'time']).to_xarray()\n",
    "\n",
    "display(SWE_stations_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe of the SWE stations\n",
    "bow_canswe_df = SWE_stations_ds.to_dataframe().reset_index()\n",
    "\n",
    "# Extract unique station coordinates\n",
    "unique_stations = bow_canswe_df.reset_index().drop_duplicates(subset='station_id')[['station_id', 'lon', 'lat']]\n",
    "\n",
    "# Convert SWE stations DataArray to GeoDataFrame for further analysis\n",
    "data = {'station_id': unique_stations['station_id'].values, \n",
    "        'lon': unique_stations['lon'].values, \n",
    "        'lat': unique_stations['lat'].values} \n",
    "df = pd.DataFrame(data)\n",
    "geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]\n",
    "crs = \"EPSG:4326\"\n",
    "SWE_stations_gdf = gpd.GeoDataFrame(df, crs=crs, geometry=geometry)\n",
    "\n",
    "display(SWE_stations_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test basin SWE data DataSet to Pandas DataFrame for further analysis\n",
    "SWE_testbasin_df = SWE_stations_ds.to_dataframe().drop(columns=['lon','lat','station_name']).unstack()['snw'].T\n",
    "\n",
    "# Remove time from dates\n",
    "SWE_testbasin_df['date'] = SWE_testbasin_df.index.normalize()\n",
    "SWE_testbasin_df = SWE_testbasin_df.set_index('date')\n",
    "\n",
    "# Drop the dates with no data at all across all stations\n",
    "SWE_testbasin_df = SWE_testbasin_df.dropna(axis=0, how='all')\n",
    "\n",
    "# Choose data for the period of interest 1980 - 2024\n",
    "SWE_testbasin_df = SWE_testbasin_df.loc['1980-01-01':'2023-07-31']\n",
    "\n",
    "display(SWE_testbasin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot timeseries of SWE station observations in the test basin\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "for s in SWE_stations_ds.station_id.values:\n",
    "    SWE_stations_ds.snw.sel(station_id = s).plot(marker='o', alpha=.3, markersize=1, label=s)\n",
    "\n",
    "plt.title('')\n",
    "plt.ylabel('SWE [mm]')\n",
    "plt.xlabel('')\n",
    "plt.xlim(pd.to_datetime('1980-01-01'), pd.to_datetime('2024-07-31'))\n",
    "plt.ylim(0, 1200)\n",
    "# Place legend at the bottom with 5 columns per line\n",
    "plt.legend(loc='lower center', bbox_to_anchor=(0.5, -0.25), ncol=10, fontsize=8, frameon=False)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data availabity monthly plots\n",
    "\n",
    "data_availability = data_availability_monthly_plots_1(SWE_stations_ds, SWE_stations_ds.snw, None, flag=0)\n",
    "# Save the plot\n",
    "data_availability.savefig(output_plots / 'data_availability_monthly_plots_1.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar charts of the days with SWE observations around the first day of each month\n",
    "fig = data_availability_monthly_plots_2(SWE_testbasin_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect CaSR SWE data to CanSWE data before linear interpolation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CaSRv3.1 data\n",
    "casr_data = pd.read_csv(CaSR_path)\n",
    "# Convert 'time' column to datetime\n",
    "casr_data['time'] = pd.to_datetime(casr_data['time'])\n",
    "# Create a DataFrame from the CaSR data\n",
    "casr_df = pd.DataFrame(casr_data)\n",
    "\n",
    "display(casr_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename Grid_id to station_id and keep SWE, 'time', 'station_id'\n",
    "casr_df = casr_df[['time', 'Grid_id','SWE']]\n",
    "casr_df.rename(columns={'Grid_id': 'station_id','time':'date'}, inplace=True)\n",
    "display(casr_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column names to ensure 'date' exists\n",
    "print(casr_df.columns)\n",
    "\n",
    "# Pivot the CaSR DataFrame to have dates as index and station_id as columns\n",
    "casr_df = casr_df.pivot(index='date', columns='station_id', values='SWE')\n",
    "\n",
    "# Optionally, rename the columns to a simple format if needed\n",
    "casr_df.columns = [f'SWE_{i+1}' for i in range(len(casr_df.columns))]\n",
    "\n",
    "display(casr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual check\n",
    "casr_df['SWE_5']['1982-09-01':'1987-10-02'].plot(marker='o', color='b')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWE_stations_ds['time'] = pd.date_range(start='1980-01-01', periods=SWE_stations_ds.dims['time'], freq='D')\n",
    "# Linear interpolation to fill in small data gaps\n",
    "SWE_testbasin_interp_da = SWE_stations_ds.snw.interpolate_na(method='linear', dim='time', max_gap=datetime.timedelta(days=max_gap_days_default))\n",
    "# Only drop columns that exist\n",
    "df_interp = SWE_testbasin_interp_da.to_dataframe()\n",
    "cols_to_drop = [col for col in ['lon', 'lat', 'station_name'] if col in df_interp.columns]\n",
    "SWE_obs_basin_interp_df = df_interp.drop(columns=cols_to_drop).unstack()['snw'].T\n",
    "SWE_obs_basin_interp_df['date'] = SWE_obs_basin_interp_df.index.normalize()\n",
    "SWE_obs_basin_interp_df = SWE_obs_basin_interp_df.set_index('date')\n",
    "display(SWE_obs_basin_interp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "SWE_obs_basin_interp_df.iloc[:,10].plot(color='r', marker='o', ms=2, label='gap filled', lw=0, alpha=.3)\n",
    "SWE_testbasin_df.iloc[:,10].plot(color='k', marker='o', ms=2, label='gap filled', lw=0, alpha=.3)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('SWE [mm]')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save flags for linear interpolation to a new Pandas dataframe (observations = 0; estimates = 1)\n",
    "flags_interp_testbasin_da = SWE_testbasin_interp_da.copy().fillna(-999)\n",
    "original_da = SWE_stations_ds.snw.copy().fillna(-999)\n",
    "flags_interp_testbasin_da = xr.where(flags_interp_testbasin_da==original_da, 0, 1)\n",
    "# Only drop columns that exist in the DataFrame\n",
    "cols_to_drop = [col for col in ['lon', 'lat', 'station_name'] if col in flags_interp_testbasin_da.to_dataframe().columns]\n",
    "flags_interp_testbasin_df = flags_interp_testbasin_da.to_dataframe().drop(columns=cols_to_drop).unstack()['snw'].T\n",
    "flags_interp_testbasin_df['date'] =  flags_interp_testbasin_df.index.normalize()\n",
    "flags_interp_testbasin_df = flags_interp_testbasin_df.set_index('date')\n",
    "display(SWE_testbasin_interp_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge test basin SWE and casr_df\n",
    "SWE_testbasin_interp_df = SWE_obs_basin_interp_df.merge(casr_df, left_index=True, right_index=True, how='outer', suffixes=('_testbasin', '_casr'))\n",
    "\n",
    "display(SWE_testbasin_interp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime(\"1980-01-01\")\n",
    "end_date = pd.to_datetime(\"2023-07-31\")\n",
    "chunk_years = 5\n",
    "\n",
    "# Initialize output dataframes\n",
    "all_SWE_obs = []\n",
    "all_flags = []\n",
    "all_donors = []\n",
    "\n",
    "current_start = start_date\n",
    "\n",
    "while current_start < end_date:\n",
    "    current_end = min(current_start + pd.DateOffset(years=chunk_years), end_date)\n",
    "    \n",
    "    print(f\"Processing chunk: {current_start.date()} to {current_end.date()}\")\n",
    "\n",
    "    chunk_df = SWE_testbasin_interp_df.loc[current_start:current_end].copy()\n",
    "    \n",
    "    SWE_obs_chunk, flags_chunk, donors_chunk = qm_gap_filling(\n",
    "        chunk_df,\n",
    "        window_days=window_days_default,\n",
    "        min_obs_corr=min_obs_corr_default,\n",
    "        min_obs_cdf=min_obs_cdf_default,\n",
    "        min_corr=min_corr_default\n",
    "    )\n",
    "\n",
    "    all_SWE_obs.append(SWE_obs_chunk)\n",
    "    all_flags.append(flags_chunk)\n",
    "    all_donors.append(donors_chunk)\n",
    "\n",
    "    current_start = current_end + pd.DateOffset(days=1)\n",
    "\n",
    "# Combine the parts\n",
    "SWE_obs_basin_gapfilled_df = pd.concat(all_SWE_obs)\n",
    "flags_gapfill_basin_df = pd.concat(all_flags)\n",
    "donor_stations_gapfill_basin_df = pd.concat(all_donors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine linear interpolation and quantile mapping flags into a single Pandas dataframe\n",
    "#if not flags_interp_basin_df.index.equals(flags_gapfill_basin_df.index) or not flags_interp_basin_df.columns.equals(flags_gapfill_basin_df.columns):\n",
    "flags_basin_df = SWE_obs_basin_interp_df + flags_gapfill_basin_df\n",
    "\n",
    "display(flags_basin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine gap filled dataset and metadata into a single dataset\n",
    "SWE_P_gapfill_testbasin_da = xr.DataArray(data=SWE_obs_basin_gapfilled_df.values, coords=dict(time=SWE_obs_basin_gapfilled_df.index.values, station_id=SWE_obs_basin_gapfilled_df.columns.values), dims=['time','station_id'], name='SWE', attrs={'long_name':'Surface snow water equivalent','units':'kg m**-2'})\n",
    "flags_testbasin_da = xr.DataArray(data=flags_basin_df.values, coords=dict(time=flags_basin_df.index.values, station_id=flags_basin_df.columns.values), dims=['time','station_id'], name='flag', attrs={'description':'observations = 0; estimates = 1'})\n",
    "donor_stations_gapfill_testbasin_da = xr.DataArray(data=donor_stations_gapfill_basin_df.values, coords=dict(time=donor_stations_gapfill_basin_df.index.values, station_id=donor_stations_gapfill_basin_df.columns.values), dims=['time','station_id'], name='donor_stations', attrs={'description':'station_id of donor stations used for gap filling'})\n",
    "SWE_testbasin_gapfilled_ds = xr.merge([SWE_P_gapfill_testbasin_da, flags_testbasin_da, donor_stations_gapfill_testbasin_da])\n",
    "# Only assign coordinates for station_ids present in SWE_stations_ds\n",
    "station_ids_all = SWE_obs_basin_gapfilled_df.columns.values\n",
    "station_ids_in_ds = [sid for sid in station_ids_all if sid in SWE_stations_ds.station_id.values]\n",
    "station_ids_not_in_ds = [sid for sid in station_ids_all if sid not in SWE_stations_ds.station_id.values]\n",
    "\n",
    "# Prepare arrays for lat, lon, and station_name, filling with NaN or empty string for missing stations\n",
    "lats = []\n",
    "lons = []\n",
    "names = []\n",
    "for sid in station_ids_all:\n",
    "\tif sid in SWE_stations_ds.station_id.values:\n",
    "\t\tlat_val = SWE_stations_ds.sel(station_id=sid).lat.values\n",
    "\t\tlon_val = SWE_stations_ds.sel(station_id=sid).lon.values\n",
    "\t\tname_val = SWE_stations_ds.sel(station_id=sid).station_name.values\n",
    "\t\t# If returned value is an array, get the first element\n",
    "\t\tif isinstance(lat_val, np.ndarray):\n",
    "\t\t\tlat_val = lat_val.item() if lat_val.size == 1 else lat_val[0]\n",
    "\t\tif isinstance(lon_val, np.ndarray):\n",
    "\t\t\tlon_val = lon_val.item() if lon_val.size == 1 else lon_val[0]\n",
    "\t\tif isinstance(name_val, np.ndarray):\n",
    "\t\t\tname_val = name_val.item() if name_val.size == 1 else name_val[0]\n",
    "\t\tlats.append(float(lat_val))\n",
    "\t\tlons.append(float(lon_val))\n",
    "\t\tnames.append(str(name_val))\n",
    "\telse:\n",
    "\t\tlats.append(np.nan)\n",
    "\t\tlons.append(np.nan)\n",
    "\t\tnames.append(\"\")\n",
    "\n",
    "SWE_testbasin_gapfilled_ds = SWE_testbasin_gapfilled_ds.assign_coords({'lat':('station_id',lats),'lon':('station_id',lons),'station_name':('station_id',names)})\n",
    "\n",
    "display(SWE_testbasin_gapfilled_ds)\n",
    "display(flags_testbasin_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first SWE station in the dataset to visually check gap filling results\n",
    "plt.figure(figsize=(8,3))\n",
    "SWE_obs_basin_gapfilled_df.iloc[:,10].plot(color='r', marker='o', ms=2, label='gap filled', lw=0, alpha=.3)\n",
    "SWE_testbasin_interp_df.iloc[:,10].plot(color='k', marker='o', ms=2, label='original', lw=0)\n",
    "plt.title(SWE_testbasin_interp_df.iloc[:,10].name)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('SWE [mm]')\n",
    "plt.legend();\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "SWE_obs_basin_gapfilled_df.iloc[:,15].plot(color='r', marker='o', ms=2, label='gap filled', lw=0, alpha=.3)\n",
    "SWE_testbasin_interp_df.iloc[:,15].plot(color='k', marker='o', ms=2, label='original', lw=0)\n",
    "plt.title(SWE_testbasin_interp_df.iloc[:,15].name)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('SWE [mm]')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a bar chart of the number of times each donor station was used for infilling\n",
    "count = []\n",
    "\n",
    "for s in SWE_testbasin_interp_df.columns.values:\n",
    "    count_s = SWE_testbasin_gapfilled_ds.donor_stations.where(SWE_testbasin_gapfilled_ds.donor_stations==s).count().data\n",
    "    count.append(count_s)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.bar(SWE_testbasin_interp_df.columns.values, count, color='b')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Donor stations')\n",
    "plt.ylabel('# times used for infilling')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot timeseries of the % of SWE stations with data in the test basin on the first day of each month, for the original data & after gap filling (flag=1)\n",
    "fig = data_availability_monthly_plots_1(SWE_stations_ds, SWE_stations_ds.snw, SWE_testbasin_gapfilled_ds.SWE, flag=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the chunk size based on your preference or system capabilities\n",
    "chunk_size = 5  # Adjust as needed based on your system's capacity\n",
    "\n",
    "# Function to process data in chunks\n",
    "def process_in_chunks(df, chunk_size):\n",
    "    chunks = (df[i:i+chunk_size] for i in range(0, len(df), chunk_size))\n",
    "    results = []\n",
    "    for chunk in chunks:\n",
    "        pd.set_option(\"mode.chained_assignment\", None)  # Suppresses the \"SettingWithCopyWarning\"\n",
    "        evaluation_artificial_gapfill_testbasin_dict, fig = artificial_gap_filling(chunk.copy(), iterations=iterations_default, artificial_gap_perc=artificial_gap_perc_default, window_days=window_days_default, min_obs_corr=min_obs_corr_default, min_obs_cdf=min_obs_cdf_default, min_corr=min_corr_default, min_obs_KGE=min_obs_KGE_default, flag=1)\n",
    "        results.append((evaluation_artificial_gapfill_testbasin_dict, fig))\n",
    "    return results\n",
    "\n",
    "# Process your large dataframe in chunks\n",
    "results = process_in_chunks(SWE_testbasin_interp_df, chunk_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the evaluations\n",
    "SWE_testbasin_evaluation_df = pd.concat(all_evaluations)\n",
    "# Combine the figures\n",
    "fig_combined = plt.figure(figsize=(15, 10))\n",
    "for i, fig in enumerate(all_figs):\n",
    "    ax = fig.get_axes()[0]  # Get the first axis of the figure\n",
    "    ax.set_position([0.1 + (i % 3) * 0.3, 0.6 - (i // 3) * 0.3, 0.25, 0.25])  # Adjust position\n",
    "    fig_combined.add_subplot(ax)\n",
    "    ax.set_title(f\"Chunk {i + 1}: {all_figs[i].axes[0].get_title()}\", fontsize=10)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "In this notebook, we've demonstrated the data preparation workflow for the Snow Drought Index package. We've loaded data, preprocessed it, extracted stations within the basin of interest, assessed data availability, and saved the processed data for use in subsequent analyses.\n",
    "\n",
    "The workflow uses the following key functions from the `data_preparation` module:\n",
    "- `load_swe_data()`, `load_precip_data()`, `load_basin_data()` for data loading\n",
    "- `preprocess_swe()`, `preprocess_precip()` for data preprocessing\n",
    "- `convert_to_geodataframe()` for converting data to GeoDataFrame\n",
    "- `extract_stations_in_basin()` for extracting stations within a basin\n",
    "- `filter_stations()` for filtering data by station\n",
    "- `assess_data_availability()` for assessing data availability\n",
    "\n",
    "These functions provide a standardized and reusable way to prepare data for the Snow Drought Index calculations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
