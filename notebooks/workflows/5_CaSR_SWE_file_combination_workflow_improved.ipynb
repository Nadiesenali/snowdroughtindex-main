{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CaSR SWE File Combination Workflow\n",
    "\n",
    "This notebook demonstrates how to combine NetCDF files from the CaSR SWE dataset using the `combine_casr_swe_files.py` script. The CaSR dataset contains files organized by variable types, spatial regions, and time periods that can be combined in different ways:\n",
    "\n",
    "1. **Temporal combination**: Combine files across time periods\n",
    "2. **Spatial combination**: Combine files across spatial regions  \n",
    "3. **Full combination**: Combine both temporal and spatial dimensions\n",
    "\n",
    "The CaSR SWE dataset includes:\n",
    "- **Variable types**: A_PR24_SFC (precipitation) and P_SWE_LAND (snow water equivalent)\n",
    "- **Spatial regions**: Different rlon/rlat coordinate ranges\n",
    "- **Time periods**: 4-year chunks from 1980-2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "**Note**: If you encounter NumPy compatibility errors, please run one of the following commands in your terminal before running this notebook:\n",
    "\n",
    "**Option 1 (Recommended)**: Install from requirements file\n",
    "```bash\n",
    "pip install -r requirements_notebook.txt\n",
    "```\n",
    "\n",
    "**Option 2**: Manual installation with compatible versions\n",
    "```bash\n",
    "pip install \"numpy<2\" xarray pandas matplotlib netcdf4\n",
    "```\n",
    "\n",
    "**Option 3**: Using conda\n",
    "```bash\n",
    "conda install numpy=1.26 xarray pandas matplotlib netcdf4\n",
    "```\n",
    "\n",
    "**Option 4**: Create a new environment with compatible versions\n",
    "```bash\n",
    "conda create -n snowdrought python=3.9 numpy=1.26 xarray pandas matplotlib netcdf4 jupyter\n",
    "conda activate snowdrought\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "xarray version: 2025.3.1\n",
      "pandas version: 2.2.3\n",
      "matplotlib version: 3.8.2\n",
      "Successfully imported CaSRFileCombiner\n",
      "Successfully imported ElevationDataFilter\n",
      "Successfully imported OptimizedElevationDataExtractor\n"
     ]
    }
   ],
   "source": [
    "# Check for NumPy compatibility issues\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='numpy')\n",
    "\n",
    "# Import required packages\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Handle NumPy compatibility\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"NumPy version: {np.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"NumPy import error: {e}\")\n",
    "    print(\"Please install NumPy: pip install numpy\")\n",
    "\n",
    "# Import data science packages with error handling\n",
    "try:\n",
    "    import xarray as xr\n",
    "    print(f\"xarray version: {xr.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"xarray import error: {e}\")\n",
    "    print(\"If you encounter NumPy compatibility issues, try:\")\n",
    "    print(\"  pip install 'numpy<2' xarray pandas matplotlib\")\n",
    "    print(\"  or\")\n",
    "    print(\"  conda install numpy=1.26 xarray pandas matplotlib\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(f\"pandas version: {pd.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"pandas import error: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(f\"matplotlib version: {plt.matplotlib.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"matplotlib import error: {e}\")\n",
    "    raise\n",
    "\n",
    "# Add the project root to Python path to import the combine script\n",
    "project_root = Path().cwd().parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import the CaSR file combiner\n",
    "try:\n",
    "    from combine_casr_swe_files import CaSRFileCombiner\n",
    "    print(\"Successfully imported CaSRFileCombiner\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing CaSRFileCombiner: {e}\")\n",
    "    print(\"Make sure combine_casr_swe_files.py is in the project root directory\")\n",
    "    raise\n",
    "\n",
    "# Import the elevation data filter (REUSING EXISTING CODE)\n",
    "try:\n",
    "    from filter_merge_elevation_data import ElevationDataFilter\n",
    "    print(\"Successfully imported ElevationDataFilter\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing ElevationDataFilter: {e}\")\n",
    "    print(\"Make sure filter_merge_elevation_data.py is in the project root directory\")\n",
    "    raise\n",
    "\n",
    "# Import the optimized elevation data extractor\n",
    "try:\n",
    "    from extract_elevation_data_optimized import OptimizedElevationDataExtractor\n",
    "    print(\"Successfully imported OptimizedElevationDataExtractor\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing OptimizedElevationDataExtractor: {e}\")\n",
    "    print(\"Make sure extract_elevation_data_optimized.py is in the project root directory\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the input and output directories for your CaSR SWE data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths - modify these paths according to your data location\n",
    "input_dir = r\"data/input_data/CaSR_SWE\"  # Directory containing CaSR NetCDF files\n",
    "output_dir = r\"data/output_data/combined_casr\"  # Directory for combined output files\n",
    "elevation_dir = r\"data/input_data/Elevation\"  # Directory containing elevation shapefiles\n",
    "filtered_output_dir = r\"data/output_data/filtered_elevation\"  # Directory for filtered elevation data\n",
    "\n",
    "# Create absolute paths\n",
    "input_path = project_root / input_dir\n",
    "output_path = project_root / output_dir\n",
    "elevation_path = project_root / elevation_dir\n",
    "filtered_output_path = project_root / filtered_output_dir\n",
    "\n",
    "print(f\"Input directory: {input_path}\")\n",
    "print(f\"Output directory: {output_path}\")\n",
    "print(f\"Elevation directory: {elevation_path}\")\n",
    "print(f\"Filtered output directory: {filtered_output_path}\")\n",
    "print(f\"Input directory exists: {input_path.exists()}\")\n",
    "print(f\"Elevation directory exists: {elevation_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the CaSR File Combiner\n",
    "\n",
    "Create an instance of the `CaSRFileCombiner` class with your input and output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 00:20:00,644 - INFO - Input directory: c:\\Users\\askha\\github\\snowdroughtindex-main-1\\data\\input_data\\CaSR_SWE\n",
      "2025-06-26 00:20:00,645 - INFO - Output directory: c:\\Users\\askha\\github\\snowdroughtindex-main-1\\data\\output_data\\combined_casr\n"
     ]
    }
   ],
   "source": [
    "# Initialize the file combiner\n",
    "combiner = CaSRFileCombiner(input_dir=str(input_path), output_dir=str(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Dataset Information\n",
    "\n",
    "Before combining files, let's examine what data is available in the input directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 00:20:02,153 - INFO - Found 2 file groups:\n",
      "2025-06-26 00:20:02,154 - INFO -   A_PR24_SFC: 44 files\n",
      "2025-06-26 00:20:02,154 - INFO -   P_SWE_LAND: 44 files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CASR SWE DATASET INFORMATION\n",
      "============================================================\n",
      "\n",
      "Group: A_PR24_SFC\n",
      "Number of files: 44\n",
      "Time coverage: 1980-2023\n",
      "Spatial regions: 4\n",
      "  rlon 211-245, rlat 386-420\n",
      "  rlon 211-245, rlat 421-455\n",
      "  rlon 246-280, rlat 386-420\n",
      "  rlon 246-280, rlat 421-455\n",
      "Variables: ['rotated_pole', 'CaSR_v3.1_A_PR24_SFC']\n",
      "Dimensions: {'time': 35064, 'rlat': 35, 'rlon': 35}\n",
      "\n",
      "Group: P_SWE_LAND\n",
      "Number of files: 44\n",
      "Time coverage: 1980-2023\n",
      "Spatial regions: 4\n",
      "  rlon 211-245, rlat 386-420\n",
      "  rlon 211-245, rlat 421-455\n",
      "  rlon 246-280, rlat 386-420\n",
      "  rlon 246-280, rlat 421-455\n",
      "Variables: ['rotated_pole', 'CaSR_v3.1_P_SWE_LAND']\n",
      "Dimensions: {'time': 35064, 'rlat': 35, 'rlon': 35}\n"
     ]
    }
   ],
   "source": [
    "# Get information about the available datasets\n",
    "combiner.get_dataset_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine File Groups\n",
    "\n",
    "Let's look at how the files are grouped by variable type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file groups\n",
    "file_groups = combiner.get_file_groups()\n",
    "\n",
    "print(\"Available file groups:\")\n",
    "for group_name, files in file_groups.items():\n",
    "    print(f\"\\n{group_name}: {len(files)} files\")\n",
    "    \n",
    "    # Show first few filenames as examples\n",
    "    for i, file_path in enumerate(files[:3]):\n",
    "        filename = Path(file_path).name\n",
    "        print(f\"  {i+1}. {filename}\")\n",
    "    \n",
    "    if len(files) > 3:\n",
    "        print(f\"  ... and {len(files) - 3} more files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Parse Individual Filenames\n",
    "\n",
    "Let's examine how the filename parsing works for understanding the file structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample file and parse its filename\n",
    "if file_groups:\n",
    "    # Get the first file from the first group\n",
    "    first_group = list(file_groups.keys())[0]\n",
    "    sample_file = file_groups[first_group][0]\n",
    "    sample_filename = Path(sample_file).name\n",
    "    \n",
    "    print(f\"Sample filename: {sample_filename}\")\n",
    "    \n",
    "    # Parse the filename\n",
    "    parsed_info = combiner.parse_filename(sample_filename)\n",
    "    \n",
    "    print(\"\\nParsed information:\")\n",
    "    for key, value in parsed_info.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"No files found in the input directory. Please check your input path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination Options\n",
    "\n",
    "Now let's demonstrate the different ways to combine the CaSR files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Temporal Combination Only\n",
    "\n",
    "Combine files across time periods while keeping spatial regions separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal combination only\n",
    "print(\"Performing temporal combination (keeping spatial regions separate)...\")\n",
    "combiner.combine_by_variable(combine_spatial=False, combine_temporal=True)\n",
    "print(\"Temporal combination completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Spatial Combination Only\n",
    "\n",
    "Combine files across spatial regions while keeping time periods separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial combination only\n",
    "print(\"Performing spatial combination (keeping time periods separate)...\")\n",
    "combiner.combine_by_variable(combine_spatial=True, combine_temporal=False)\n",
    "print(\"Spatial combination completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Full Combination\n",
    "\n",
    "Combine files across both spatial and temporal dimensions to create complete datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full combination (both spatial and temporal)\n",
    "print(\"Performing full combination (both spatial and temporal)...\")\n",
    "combiner.combine_by_variable(combine_spatial=True, combine_temporal=True)\n",
    "print(\"Full combination completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Combined Output Files\n",
    "\n",
    "Let's check what files were created in the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List output files\n",
    "output_files = list(output_path.glob('*.nc'))\n",
    "\n",
    "print(f\"Combined files created in {output_path}:\")\n",
    "print(f\"Total files: {len(output_files)}\\n\")\n",
    "\n",
    "for i, file_path in enumerate(output_files, 1):\n",
    "    file_size = file_path.stat().st_size / (1024**2)  # Size in MB\n",
    "    print(f\"{i}. {file_path.name} ({file_size:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Elevation Data from Combined Files\n",
    "\n",
    "Now that we have combined the CaSR SWE files, let's extract data at specific elevation points using the optimized elevation data extractor. This allows us to track SWE and precipitation data at different elevation levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Elevation Data Extraction\n",
    "\n",
    "Set up the paths for elevation data and configure extraction parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elevation output directory: c:\\Users\\askha\\github\\snowdroughtindex-main-1\\data\\output_data\\elevation\n"
     ]
    }
   ],
   "source": [
    "# Define elevation data paths\n",
    "elevation_output_dir = r\"data/output_data/elevation\"  # Directory for elevation extraction output\n",
    "\n",
    "# Create absolute paths\n",
    "elevation_output_path = project_root / elevation_output_dir\n",
    "\n",
    "print(f\"Elevation output directory: {elevation_output_path}\")\n",
    "\n",
    "# Initialize the elevation data extractor\n",
    "elevation_extractor = OptimizedElevationDataExtractor(\n",
    "    elevation_dir=str(elevation_path),\n",
    "    combined_casr_dir=str(output_path),  # Use the output from CaSR combination\n",
    "    output_dir=str(elevation_output_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Explore Elevation Data\n",
    "\n",
    "Load the elevation shapefile to see what elevation points are available for data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 00:20:25,053 - INFO - Loading elevation data...\n",
      "2025-06-26 00:20:25,054 - INFO - Loading shapefile: c:\\Users\\askha\\github\\snowdroughtindex-main-1\\data\\input_data\\Elevation\\Bow_elevation_combined.shp\n",
      "2025-06-26 00:20:25,061 - INFO - Loaded 13 elevation points\n",
      "2025-06-26 00:20:25,062 - INFO - Elevation data columns: ['PROVCD_1', 'VALDATE', 'EDITION', 'DATASETNAM', 'VERSION', 'COMPLEVEL', 'WSCMDA', 'WSCSDA', 'WSCSSDA', 'FDA', 'OCEAN', 'WSCMDANAME', 'WSCSDANAME', 'WSCSSDANAM', 'min', 'max', 'mean', 'count', 'std', 'median', 'PROVCD_2', 'elev_class', 'geometry']\n",
      "2025-06-26 00:20:25,062 - INFO - CRS: None\n",
      "2025-06-26 00:20:25,063 - INFO - Elevation-related columns: ['min', 'max', 'mean', 'median', 'elev_class']\n",
      "2025-06-26 00:20:25,063 - INFO - min range: 0.0 - 1440.0\n",
      "2025-06-26 00:20:25,064 - INFO - max range: 998.0 - 3490.0\n",
      "2025-06-26 00:20:25,065 - INFO - mean range: 772.4 - 2163.8\n",
      "2025-06-26 00:20:25,066 - INFO - median range: 764.0 - 2182.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total elevation points: 13\n",
      "\n",
      "First 5 elevation points:\n",
      "  PROVCD_1   VALDATE EDITION DATASETNAM VERSION COMPLEVEL WSCMDA WSCSDA  \\\n",
      "0       AB  20070208       1    05BM000       0   NHN-CL1     05    05B   \n",
      "1       AB  20070130       1    05BN000       0   NHN-CL1     05    05B   \n",
      "2       AB  20070226       1    05BH000       0   NHN-CL1     05    05B   \n",
      "3       AB  20070228       1    05BK000       0   NHN-CL1     05    05B   \n",
      "4       AB  20070302       1    05BE000       0   NHN-CL1     05    05B   \n",
      "\n",
      "  WSCSSDA   FDA  ...                 WSCSSDANAM     min     max         mean  \\\n",
      "0    05BM  05BM  ...       Lower Bow - Crowfoot   776.0  1177.0   953.688214   \n",
      "1    05BN  05BN  ...          Lower Bow - Mouth   700.0   998.0   772.376015   \n",
      "2    05BH  05BH  ...  Central Bow - Jumpingpond  1038.0  2479.0  1259.913722   \n",
      "3    05BK  05BK  ...               Fish (Alta.)   981.0  1777.0  1226.170423   \n",
      "4    05BE  05BE  ...      Upper Bow - Policeman  1146.0  3054.0  1601.837836   \n",
      "\n",
      "     count         std  median  PROVCD_2  elev_class  \\\n",
      "0  9897452   71.132927   944.0      None   500_1000m   \n",
      "1  9312616   32.852467   764.0      None   500_1000m   \n",
      "2  4121228  176.434021  1227.0      None  1000_1500m   \n",
      "3  1151005  126.086335  1194.0      None  1000_1500m   \n",
      "4  1655101  376.052387  1437.0      None  1500_2000m   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((-112.58577 51.23024, -112.58519 51.2...  \n",
      "1  POLYGON ((-112.4962 50.71881, -112.49566 50.71...  \n",
      "2  POLYGON ((-113.93263 51.12563, -113.93288 51.1...  \n",
      "3  POLYGON ((-114.10188 50.95504, -114.0975 50.95...  \n",
      "4  POLYGON ((-114.72215 51.33033, -114.72235 51.3...  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "Elevation statistics:\n",
      "  min:\n",
      "    Min: 0.0\n",
      "    Max: 1440.0\n",
      "    Mean: 1024.1\n",
      "  max:\n",
      "    Min: 998.0\n",
      "    Max: 3490.0\n",
      "    Mean: 2746.8\n",
      "  mean:\n",
      "    Min: 772.4\n",
      "    Max: 2163.8\n",
      "    Mean: 1639.1\n",
      "  median:\n",
      "    Min: 764.0\n",
      "    Max: 2182.0\n",
      "    Mean: 1601.1\n"
     ]
    }
   ],
   "source": [
    "# Load elevation data\n",
    "elevation_extractor.load_elevation_data()\n",
    "\n",
    "# Display basic information about elevation points\n",
    "if elevation_extractor.elevation_gdf is not None:\n",
    "    print(f\"\\nTotal elevation points: {len(elevation_extractor.elevation_gdf)}\")\n",
    "    print(f\"\\nFirst 5 elevation points:\")\n",
    "    print(elevation_extractor.elevation_gdf.head())\n",
    "    \n",
    "    # Show elevation statistics if available\n",
    "    elev_cols = [col for col in elevation_extractor.elevation_gdf.columns \n",
    "                 if 'elev' in col.lower() or col in ['min', 'max', 'mean', 'median']]\n",
    "    if elev_cols:\n",
    "        print(f\"\\nElevation statistics:\")\n",
    "        for col in elev_cols:\n",
    "            if pd.api.types.is_numeric_dtype(elevation_extractor.elevation_gdf[col]):\n",
    "                print(f\"  {col}:\")\n",
    "                print(f\"    Min: {elevation_extractor.elevation_gdf[col].min():.1f}\")\n",
    "                print(f\"    Max: {elevation_extractor.elevation_gdf[col].max():.1f}\")\n",
    "                print(f\"    Mean: {elevation_extractor.elevation_gdf[col].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Available Combined CaSR Files\n",
    "\n",
    "Let's see what combined CaSR files are available for elevation data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 00:20:44,528 - INFO - Found 0 temporal combined files\n",
      "2025-06-26 00:20:44,529 - INFO - Found 2 full combined files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available files for elevation extraction:\n",
      "\n",
      "Temporal combined files (0):\n",
      "\n",
      "Full combined files (2):\n",
      "  1. CaSR_v3.1_A_PR24_SFC_combined_full.nc\n",
      "  2. CaSR_v3.1_P_SWE_LAND_combined_full.nc\n"
     ]
    }
   ],
   "source": [
    "# Get available combined CaSR files\n",
    "temporal_files, full_files = elevation_extractor.get_combined_casr_files()\n",
    "\n",
    "print(\"Available files for elevation extraction:\")\n",
    "print(f\"\\nTemporal combined files ({len(temporal_files)}):\")\n",
    "for i, file in enumerate(temporal_files[:3], 1):\n",
    "    print(f\"  {i}. {file.name}\")\n",
    "if len(temporal_files) > 3:\n",
    "    print(f\"  ... and {len(temporal_files) - 3} more files\")\n",
    "\n",
    "print(f\"\\nFull combined files ({len(full_files)}):\")\n",
    "for i, file in enumerate(full_files[:3], 1):\n",
    "    print(f\"  {i}. {file.name}\")\n",
    "if len(full_files) > 3:\n",
    "    print(f\"  ... and {len(full_files) - 3} more files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Elevation Data with Optimization\n",
    "\n",
    "Extract data at elevation points from the combined CaSR files. We'll use time sampling to handle large datasets efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 00:21:11,377 - INFO - Found 0 temporal combined files\n",
      "2025-06-26 00:21:11,378 - INFO - Found 2 full combined files\n",
      "2025-06-26 00:21:11,378 - INFO - Processing temporal combined files...\n",
      "2025-06-26 00:21:11,379 - INFO - Processing full combined files...\n",
      "2025-06-26 00:21:11,379 - INFO - Processing CaSR_v3.1_A_PR24_SFC_combined_full.nc...\n",
      "2025-06-26 00:21:11,396 - INFO - Dataset variables: ['rotated_pole', 'CaSR_v3.1_A_PR24_SFC']\n",
      "2025-06-26 00:21:11,398 - INFO - Dataset dimensions: {'time': 385704, 'rlon': 70, 'rlat': 70}\n",
      "2025-06-26 00:21:11,400 - INFO - Time dimension size: 385704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction configuration:\n",
      "  Time sampling: all\n",
      "  Max records per point: 10000\n",
      "  File types to process: ['temporal', 'full']\n",
      "\n",
      "Starting elevation data extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 00:22:39,939 - INFO - Extracted data for 5014152 records from 13 points\n",
      "2025-06-26 00:22:40,776 - INFO - Processing CaSR_v3.1_P_SWE_LAND_combined_full.nc...\n",
      "2025-06-26 00:22:40,795 - INFO - Dataset variables: ['rotated_pole', 'CaSR_v3.1_P_SWE_LAND']\n",
      "2025-06-26 00:22:40,797 - INFO - Dataset dimensions: {'time': 385704, 'rlon': 70, 'rlat': 70}\n",
      "2025-06-26 00:22:40,800 - INFO - Time dimension size: 385704\n",
      "2025-06-26 00:24:23,421 - INFO - Extracted data for 5014152 records from 13 points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extraction completed!\n",
      "Processed 2 file groups\n"
     ]
    }
   ],
   "source": [
    "# Configure extraction parameters\n",
    "time_sampling = 'all'  # Options: 'all', 'monthly', 'yearly', 'sample'\n",
    "max_records = 10000  # Maximum records per point to avoid memory issues\n",
    "file_types = ['temporal', 'full']  # Which file types to process\n",
    "\n",
    "print(f\"Extraction configuration:\")\n",
    "print(f\"  Time sampling: {time_sampling}\")\n",
    "print(f\"  Max records per point: {max_records}\")\n",
    "print(f\"  File types to process: {file_types}\")\n",
    "print(f\"\\nStarting elevation data extraction...\")\n",
    "\n",
    "# Process all files and extract elevation data\n",
    "extraction_results = elevation_extractor.process_all_files(\n",
    "    file_types=file_types,\n",
    "    time_sampling=time_sampling,\n",
    "    max_records=max_records\n",
    ")\n",
    "\n",
    "print(f\"\\nExtraction completed!\")\n",
    "print(f\"Processed {len(extraction_results)} file groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Extracted Data\n",
    "\n",
    "Save the extracted elevation data to files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 00:25:28,200 - INFO - Saving results to c:\\Users\\askha\\github\\snowdroughtindex-main-1\\data\\output_data\\elevation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving extracted data in csv format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 00:26:08,397 - INFO - Saved CSV: c:\\Users\\askha\\github\\snowdroughtindex-main-1\\data\\output_data\\elevation\\elevation_extracted_full_CaSR_v3.1_A_PR24_SFC_combined_full.csv\n",
      "2025-06-26 00:26:51,229 - INFO - Saved CSV: c:\\Users\\askha\\github\\snowdroughtindex-main-1\\data\\output_data\\elevation\\elevation_extracted_full_CaSR_v3.1_P_SWE_LAND_combined_full.csv\n",
      "2025-06-26 00:26:51,230 - INFO - Generating summary report...\n",
      "2025-06-26 00:26:51,320 - INFO - Summary saved to: c:\\Users\\askha\\github\\snowdroughtindex-main-1\\data\\output_data\\elevation\\extraction_summary_optimized.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OPTIMIZED EXTRACTION SUMMARY\n",
      "============================================================\n",
      "Elevation points processed: 13\n",
      "Files processed: 2\n",
      "Total records extracted: 10028304\n",
      "\n",
      "File details:\n",
      "  full_CaSR_v3.1_A_PR24_SFC_combined_full:\n",
      "    Records: 5014152\n",
      "    Variables: elevation_min, elevation_max, elevation_mean, elevation_median, CaSR_v3.1_A_PR24_SFC\n",
      "    Time range: 1979-12-31T13:00:00 to 2023-12-31T12:00:00\n",
      "  full_CaSR_v3.1_P_SWE_LAND_combined_full:\n",
      "    Records: 5014152\n",
      "    Variables: elevation_min, elevation_max, elevation_mean, elevation_median, CaSR_v3.1_P_SWE_LAND\n",
      "    Time range: 1979-12-31T13:00:00 to 2023-12-31T12:00:00\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Save results in multiple formats\n",
    "output_format = 'csv'  # Options: 'csv', 'parquet', 'both'\n",
    "\n",
    "print(f\"Saving extracted data in {output_format} format...\")\n",
    "elevation_extractor.save_results(extraction_results, format=output_format)\n",
    "\n",
    "# Generate summary report\n",
    "elevation_extractor.generate_summary_report(extraction_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and Merge Elevation Data\n",
    "\n",
    "Now we'll use the existing `ElevationDataFilter` class from `filter_merge_elevation_data.py` to filter and merge the elevation data with non-null precipitation and SWE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElevationDataFilter initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize the elevation data filter using the existing class\n",
    "elevation_filter = ElevationDataFilter(\n",
    "    elevation_dir=str(elevation_path),\n",
    "    casr_dir=str(output_path),  # Use combined CaSR files\n",
    "    output_dir=str(filtered_output_path)\n",
    ")\n",
    "\n",
    "print(\"ElevationDataFilter initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Elevation Data with Filtering\n",
    "\n",
    "Use the main processing function to filter and merge elevation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 00:27:08,326 - INFO - Loading elevation data...\n",
      "2025-06-26 00:27:08,328 - INFO - Loading shapefile: c:\\Users\\askha\\github\\snowdroughtindex-main-1\\data\\input_data\\Elevation\\Bow_elevation_combined.shp\n",
      "2025-06-26 00:27:08,334 - INFO - Loaded 13 elevation points\n",
      "2025-06-26 00:27:08,335 - INFO - Elevation data columns: ['PROVCD_1', 'VALDATE', 'EDITION', 'DATASETNAM', 'VERSION', 'COMPLEVEL', 'WSCMDA', 'WSCSDA', 'WSCSSDA', 'FDA', 'OCEAN', 'WSCMDANAME', 'WSCSDANAME', 'WSCSSDANAM', 'min', 'max', 'mean', 'count', 'std', 'median', 'PROVCD_2', 'elev_class', 'geometry']\n",
      "2025-06-26 00:27:08,335 - INFO - Elevation columns found: ['min', 'max', 'mean', 'median', 'elev_class']\n",
      "2025-06-26 00:27:08,336 - INFO - min range: 0.0 - 1440.0\n",
      "2025-06-26 00:27:08,337 - INFO - max range: 998.0 - 3490.0\n",
      "2025-06-26 00:27:08,337 - INFO - mean range: 772.4 - 2163.8\n",
      "2025-06-26 00:27:08,338 - INFO - median range: 764.0 - 2182.0\n",
      "2025-06-26 00:27:08,339 - INFO - Found 1 precipitation files\n",
      "2025-06-26 00:27:08,339 - INFO - Found 1 SWE files\n",
      "2025-06-26 00:27:08,339 - INFO - Processing sample files...\n",
      "2025-06-26 00:27:08,340 - INFO - Extracting precipitation data from CaSR_v3.1_A_PR24_SFC_combined_full.nc...\n",
      "2025-06-26 00:27:08,356 - INFO - Extracting variable: CaSR_v3.1_A_PR24_SFC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing configuration:\n",
      "  Sample points: 0\n",
      "  Sample time steps: 0\n",
      "\n",
      "Starting elevation data filtering and merging process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 00:30:19,868 - INFO - Extracted 5014152 records for precipitation\n",
      "2025-06-26 00:30:20,552 - INFO - Extracting swe data from CaSR_v3.1_P_SWE_LAND_combined_full.nc...\n",
      "2025-06-26 00:30:20,566 - INFO - Extracting variable: CaSR_v3.1_P_SWE_LAND\n",
      "2025-06-26 00:33:33,321 - INFO - Extracted 5014152 records for swe\n",
      "2025-06-26 00:33:34,134 - INFO - Filtering and merging data...\n",
      "2025-06-26 00:33:35,719 - INFO - Total merged records: 5014152\n",
      "2025-06-26 00:33:35,725 - INFO - Records with null precipitation: 4805229\n",
      "2025-06-26 00:33:35,732 - INFO - Records with null SWE: 4805229\n",
      "2025-06-26 00:33:35,777 - INFO - Records with non-null values in both variables: 208923\n",
      "2025-06-26 00:33:35,789 - INFO - Analyzing elevation patterns...\n",
      "2025-06-26 00:33:35,816 - INFO - Correlation between precipitation and SWE: -0.012\n",
      "2025-06-26 00:33:35,817 - INFO - Saving results to c:\\Users\\askha\\github\\snowdroughtindex-main-1\\data\\output_data\\filtered_elevation\n",
      "2025-06-26 00:33:37,183 - INFO - Saved filtered data to: c:\\Users\\askha\\github\\snowdroughtindex-main-1\\data\\output_data\\filtered_elevation\\filtered_elevation_data.csv\n",
      "2025-06-26 00:33:37,185 - INFO - Saved statistics to: c:\\Users\\askha\\github\\snowdroughtindex-main-1\\data\\output_data\\filtered_elevation\\elevation_statistics.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during processing: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
      "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
      "Trying to import the above resulted in these errors:\n",
      " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Process the data using the existing functionality\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[43melevation_filter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_points\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_time\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mElevation data filtering and merging completed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\askha\\github\\snowdroughtindex-main-1\\filter_merge_elevation_data.py:507\u001b[39m, in \u001b[36mElevationDataFilter.process\u001b[39m\u001b[34m(self, sample_points, sample_time)\u001b[39m\n\u001b[32m    504\u001b[39m stats_df = \u001b[38;5;28mself\u001b[39m.analyze_elevation_patterns(filtered_df, precip_col, swe_col)\n\u001b[32m    506\u001b[39m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mboth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\askha\\github\\snowdroughtindex-main-1\\filter_merge_elevation_data.py:406\u001b[39m, in \u001b[36mElevationDataFilter.save_results\u001b[39m\u001b[34m(self, filtered_df, stats_df, format)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mparquet\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mboth\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m    405\u001b[39m     parquet_file = \u001b[38;5;28mself\u001b[39m.output_dir / \u001b[33m\"\u001b[39m\u001b[33mfiltered_elevation_data.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m     \u001b[43mfiltered_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparquet_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved filtered data to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparquet_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stats_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py:3113\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3032\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3033\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3034\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3109\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3110\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3111\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3113\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3114\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3121\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3122\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parquet.py:476\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partition_cols, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    475\u001b[39m     partition_cols = [partition_cols]\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m    480\u001b[39m impl.write(\n\u001b[32m    481\u001b[39m     df,\n\u001b[32m    482\u001b[39m     path_or_buf,\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m     **kwargs,\n\u001b[32m    489\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parquet.py:67\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     64\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     65\u001b[39m             error_msgs += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     68\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to find a usable engine; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     69\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtried using: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfastparquet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA suitable version of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     71\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTrying to import the above resulted in these errors:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m     )\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[31mImportError\u001b[39m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "# Configure processing parameters\n",
    "sample_points = 0  # Number of elevation points to sample for testing\n",
    "sample_time = 0     # Number of time steps to sample for testing\n",
    "\n",
    "print(f\"Processing configuration:\")\n",
    "print(f\"  Sample points: {sample_points}\")\n",
    "print(f\"  Sample time steps: {sample_time}\")\n",
    "print(f\"\\nStarting elevation data filtering and merging process...\")\n",
    "\n",
    "# Process the data using the existing functionality\n",
    "try:\n",
    "    elevation_filter.process(\n",
    "        sample_points=sample_points,\n",
    "        sample_time=sample_time\n",
    "    )\n",
    "    print(\"\\nElevation data filtering and merging completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during processing: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Filtered Results\n",
    "\n",
    "Let's check what files were created by the filtering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List filtered output files\n",
    "filtered_files = list(filtered_output_path.glob('*'))\n",
    "\n",
    "print(f\"Filtered files created in {filtered_output_path}:\")\n",
    "print(f\"Total files: {len(filtered_files)}\\n\")\n",
    "\n",
    "for i, file_path in enumerate(filtered_files, 1):\n",
    "    if file_path.is_file():\n",
    "        file_size = file_path.stat().st_size / 1024  # Size in KB\n",
    "        print(f\"{i}. {file_path.name} ({file_size:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Examine Filtered Data\n",
    "\n",
    "Let's load and examine the filtered elevation data to understand the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the filtered data if it exists\n",
    "filtered_csv = filtered_output_path / \"filtered_elevation_data.csv\"\n",
    "stats_csv = filtered_output_path / \"elevation_statistics.csv\"\n",
    "\n",
    "if filtered_csv.exists():\n",
    "    # Load filtered data\n",
    "    filtered_df = pd.read_csv(filtered_csv)\n",
    "    print(f\"Loaded filtered elevation data: {len(filtered_df)} records\")\n",
    "    print(f\"\\nData columns: {list(filtered_df.columns)}\")\n",
    "    print(f\"\\nFirst 5 records:\")\n",
    "    print(filtered_df.head())\n",
    "    \n",
    "    # Show basic statistics\n",
    "    print(f\"\\nBasic statistics:\")\n",
    "    numeric_cols = filtered_df.select_dtypes(include=[np.number]).columns\n",
    "    print(filtered_df[numeric_cols].describe())\n",
    "    \n",
    "    # Load elevation statistics if available\n",
    "    if stats_csv.exists():\n",
    "        stats_df = pd.read_csv(stats_csv, index_col=0)\n",
    "        print(f\"\\nElevation statistics by elevation bins:\")\n",
    "        print(stats_df)\n",
    "else:\n",
    "    print(\"No filtered data file found. Check if the processing completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated the complete workflow for:\n",
    "\n",
    "1. **CaSR File Combination**: Combined NetCDF files across temporal and spatial dimensions using three different approaches:\n",
    "   - Temporal combination only (keeping spatial regions separate)\n",
    "   - Spatial combination only (keeping time periods separate)\n",
    "   - Full combination (both spatial and temporal dimensions)\n",
    "\n",
    "2. **Elevation Data Extraction**: Used the `OptimizedElevationDataExtractor` to extract precipitation and SWE data at specific elevation points from the combined CaSR files\n",
    "\n",
    "3. **Data Filtering and Merging**: Used the existing `ElevationDataFilter` class to create a clean dataset with only non-null precipitation and SWE values\n",
    "\n",
    "4. **Elevation Pattern Analysis**: Analyzed relationships between elevation and climate variables\n",
    "\n",
    "5. **Data Export**: Saved the filtered and merged dataset for further analysis\n",
    "\n",
    "The final output includes:\n",
    "- **Combined CaSR files**: NetCDF files with temporal, spatial, or full combinations\n",
    "- **Extracted elevation data**: CSV/Parquet files with climate data at elevation points\n",
    "- **filtered_elevation_data.csv/parquet**: Main dataset with non-null precipitation and SWE values\n",
    "- **elevation_statistics.csv/parquet**: Statistical analysis by elevation bins\n",
    "- **filtering_summary.json**: Summary report of the filtering process\n",
    "\n",
    "This workflow provides a foundation for analyzing snow drought patterns across different elevation zones using the CaSR dataset.\n",
    "\n",
    "### Key Improvement\n",
    "\n",
    "This improved version **reuses existing functionality** from both `extract_elevation_data_optimized.py` and `filter_merge_elevation_data.py` instead of duplicating code in the notebook. This approach:\n",
    "\n",
    "- **Eliminates code duplication** (removed ~200+ lines of redundant code)\n",
    "- **Maintains consistency** across the project\n",
    "- **Makes maintenance easier** - updates to logic only need to be made in one place\n",
    "- **Follows DRY (Don't Repeat Yourself) principles**\n",
    "- **Reduces notebook complexity** and focuses on the workflow rather than implementation details\n",
    "- **Includes all original functionality** while being more maintainable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
