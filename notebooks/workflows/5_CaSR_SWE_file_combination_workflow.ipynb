{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CaSR SWE File Combination Workflow\n",
    "\n",
    "This notebook demonstrates how to combine NetCDF files from the CaSR SWE dataset using the `combine_casr_swe_files.py` script. The CaSR dataset contains files organized by variable types, spatial regions, and time periods that can be combined in different ways:\n",
    "\n",
    "1. **Temporal combination**: Combine files across time periods\n",
    "2. **Spatial combination**: Combine files across spatial regions  \n",
    "3. **Full combination**: Combine both temporal and spatial dimensions\n",
    "\n",
    "The CaSR SWE dataset includes:\n",
    "- **Variable types**: A_PR24_SFC (precipitation) and P_SWE_LAND (snow water equivalent)\n",
    "- **Spatial regions**: Different rlon/rlat coordinate ranges\n",
    "- **Time periods**: 4-year chunks from 1980-2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "**Note**: If you encounter NumPy compatibility errors, please run one of the following commands in your terminal before running this notebook:\n",
    "\n",
    "**Option 1 (Recommended)**: Install from requirements file\n",
    "```bash\n",
    "pip install -r requirements_notebook.txt\n",
    "```\n",
    "\n",
    "**Option 2**: Manual installation with compatible versions\n",
    "```bash\n",
    "pip install \"numpy<2\" xarray pandas matplotlib netcdf4\n",
    "```\n",
    "\n",
    "**Option 3**: Using conda\n",
    "```bash\n",
    "conda install numpy=1.26 xarray pandas matplotlib netcdf4\n",
    "```\n",
    "\n",
    "**Option 4**: Create a new environment with compatible versions\n",
    "```bash\n",
    "conda create -n snowdrought python=3.9 numpy=1.26 xarray pandas matplotlib netcdf4 jupyter\n",
    "conda activate snowdrought\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NumPy compatibility issues\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='numpy')\n",
    "\n",
    "# Import required packages\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Handle NumPy compatibility\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"NumPy version: {np.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"NumPy import error: {e}\")\n",
    "    print(\"Please install NumPy: pip install numpy\")\n",
    "\n",
    "# Import data science packages with error handling\n",
    "try:\n",
    "    import xarray as xr\n",
    "    print(f\"xarray version: {xr.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"xarray import error: {e}\")\n",
    "    print(\"If you encounter NumPy compatibility issues, try:\")\n",
    "    print(\"  pip install 'numpy<2' xarray pandas matplotlib\")\n",
    "    print(\"  or\")\n",
    "    print(\"  conda install numpy=1.26 xarray pandas matplotlib\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(f\"pandas version: {pd.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"pandas import error: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(f\"matplotlib version: {plt.matplotlib.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"matplotlib import error: {e}\")\n",
    "    raise\n",
    "\n",
    "# Add the project root to Python path to import the combine script\n",
    "project_root = Path().cwd().parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import the CaSR file combiner\n",
    "try:\n",
    "    from combine_casr_swe_files import CaSRFileCombiner\n",
    "    print(\"Successfully imported CaSRFileCombiner\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing CaSRFileCombiner: {e}\")\n",
    "    print(\"Make sure combine_casr_swe_files.py is in the project root directory\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the input and output directories for your CaSR SWE data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths - modify these paths according to your data location\n",
    "input_dir = r\"data/input_data/CaSR_SWE\"  # Directory containing CaSR NetCDF files\n",
    "output_dir = r\"data/output_data/combined_casr\"  # Directory for combined output files\n",
    "\n",
    "# Create absolute paths\n",
    "input_path = project_root / input_dir\n",
    "output_path = project_root / output_dir\n",
    "\n",
    "print(f\"Input directory: {input_path}\")\n",
    "print(f\"Output directory: {output_path}\")\n",
    "print(f\"Input directory exists: {input_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the CaSR File Combiner\n",
    "\n",
    "Create an instance of the `CaSRFileCombiner` class with your input and output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the file combiner\n",
    "combiner = CaSRFileCombiner(input_dir=str(input_path), output_dir=str(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Dataset Information\n",
    "\n",
    "Before combining files, let's examine what data is available in the input directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the available datasets\n",
    "combiner.get_dataset_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine File Groups\n",
    "\n",
    "Let's look at how the files are grouped by variable type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file groups\n",
    "file_groups = combiner.get_file_groups()\n",
    "\n",
    "print(\"Available file groups:\")\n",
    "for group_name, files in file_groups.items():\n",
    "    print(f\"\\n{group_name}: {len(files)} files\")\n",
    "    \n",
    "    # Show first few filenames as examples\n",
    "    for i, file_path in enumerate(files[:3]):\n",
    "        filename = Path(file_path).name\n",
    "        print(f\"  {i+1}. {filename}\")\n",
    "    \n",
    "    if len(files) > 3:\n",
    "        print(f\"  ... and {len(files) - 3} more files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Parse Individual Filenames\n",
    "\n",
    "Let's examine how the filename parsing works for understanding the file structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample file and parse its filename\n",
    "if file_groups:\n",
    "    # Get the first file from the first group\n",
    "    first_group = list(file_groups.keys())[0]\n",
    "    sample_file = file_groups[first_group][0]\n",
    "    sample_filename = Path(sample_file).name\n",
    "    \n",
    "    print(f\"Sample filename: {sample_filename}\")\n",
    "    \n",
    "    # Parse the filename\n",
    "    parsed_info = combiner.parse_filename(sample_filename)\n",
    "    \n",
    "    print(\"\\nParsed information:\")\n",
    "    for key, value in parsed_info.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"No files found in the input directory. Please check your input path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination Options\n",
    "\n",
    "Now let's demonstrate the different ways to combine the CaSR files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Temporal Combination Only\n",
    "\n",
    "Combine files across time periods while keeping spatial regions separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal combination only\n",
    "print(\"Performing temporal combination (keeping spatial regions separate)...\")\n",
    "combiner.combine_by_variable(combine_spatial=False, combine_temporal=True)\n",
    "print(\"Temporal combination completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Spatial Combination Only\n",
    "\n",
    "Combine files across spatial regions while keeping time periods separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial combination only\n",
    "print(\"Performing spatial combination (keeping time periods separate)...\")\n",
    "combiner.combine_by_variable(combine_spatial=True, combine_temporal=False)\n",
    "print(\"Spatial combination completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Full Combination\n",
    "\n",
    "Combine files across both spatial and temporal dimensions to create complete datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full combination (both spatial and temporal)\n",
    "print(\"Performing full combination (both spatial and temporal)...\")\n",
    "combiner.combine_by_variable(combine_spatial=True, combine_temporal=True)\n",
    "print(\"Full combination completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Combined Output Files\n",
    "\n",
    "Let's check what files were created in the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List output files\n",
    "output_files = list(output_path.glob('*.nc'))\n",
    "\n",
    "print(f\"Combined files created in {output_path}:\")\n",
    "print(f\"Total files: {len(output_files)}\\n\")\n",
    "\n",
    "for i, file_path in enumerate(output_files, 1):\n",
    "    file_size = file_path.stat().st_size / (1024**2)  # Size in MB\n",
    "    print(f\"{i}. {file_path.name} ({file_size:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Elevation Data from Combined Files\n",
    "\n",
    "Now that we have combined the CaSR SWE files, let's extract data at specific elevation points using the optimized elevation data extractor. This allows us to track SWE and precipitation data at different elevation levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the optimized elevation data extractor\n",
    "try:\n",
    "    from extract_elevation_data_optimized import OptimizedElevationDataExtractor\n",
    "    print(\"Successfully imported OptimizedElevationDataExtractor\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing OptimizedElevationDataExtractor: {e}\")\n",
    "    print(\"Make sure extract_elevation_data_optimized.py is in the project root directory\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Elevation Data Extraction\n",
    "\n",
    "Set up the paths for elevation data and configure extraction parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define elevation data paths\n",
    "elevation_dir = r\"data/input_data/Elevation\"  # Directory containing elevation shapefiles\n",
    "elevation_output_dir = r\"data/output_data/elevation\"  # Directory for elevation extraction output\n",
    "\n",
    "# Create absolute paths\n",
    "elevation_path = project_root / elevation_dir\n",
    "elevation_output_path = project_root / elevation_output_dir\n",
    "\n",
    "print(f\"Elevation data directory: {elevation_path}\")\n",
    "print(f\"Elevation output directory: {elevation_output_path}\")\n",
    "print(f\"Elevation directory exists: {elevation_path.exists()}\")\n",
    "\n",
    "# Initialize the elevation data extractor\n",
    "elevation_extractor = OptimizedElevationDataExtractor(\n",
    "    elevation_dir=str(elevation_path),\n",
    "    combined_casr_dir=str(output_path),  # Use the output from CaSR combination\n",
    "    output_dir=str(elevation_output_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Explore Elevation Data\n",
    "\n",
    "Load the elevation shapefile to see what elevation points are available for data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load elevation data\n",
    "elevation_extractor.load_elevation_data()\n",
    "\n",
    "# Display basic information about elevation points\n",
    "if elevation_extractor.elevation_gdf is not None:\n",
    "    print(f\"\\nTotal elevation points: {len(elevation_extractor.elevation_gdf)}\")\n",
    "    print(f\"\\nFirst 5 elevation points:\")\n",
    "    print(elevation_extractor.elevation_gdf.head())\n",
    "    \n",
    "    # Show elevation statistics if available\n",
    "    elev_cols = [col for col in elevation_extractor.elevation_gdf.columns \n",
    "                 if 'elev' in col.lower() or col in ['min', 'max', 'mean', 'median']]\n",
    "    if elev_cols:\n",
    "        print(f\"\\nElevation statistics:\")\n",
    "        for col in elev_cols:\n",
    "            if pd.api.types.is_numeric_dtype(elevation_extractor.elevation_gdf[col]):\n",
    "                print(f\"  {col}:\")\n",
    "                print(f\"    Min: {elevation_extractor.elevation_gdf[col].min():.1f}\")\n",
    "                print(f\"    Max: {elevation_extractor.elevation_gdf[col].max():.1f}\")\n",
    "                print(f\"    Mean: {elevation_extractor.elevation_gdf[col].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Available Combined CaSR Files\n",
    "\n",
    "Let's see what combined CaSR files are available for elevation data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get available combined CaSR files\n",
    "temporal_files, full_files = elevation_extractor.get_combined_casr_files()\n",
    "\n",
    "print(\"Available files for elevation extraction:\")\n",
    "print(f\"\\nTemporal combined files ({len(temporal_files)}):\")\n",
    "for i, file in enumerate(temporal_files[:3], 1):\n",
    "    print(f\"  {i}. {file.name}\")\n",
    "if len(temporal_files) > 3:\n",
    "    print(f\"  ... and {len(temporal_files) - 3} more files\")\n",
    "\n",
    "print(f\"\\nFull combined files ({len(full_files)}):\")\n",
    "for i, file in enumerate(full_files[:3], 1):\n",
    "    print(f\"  {i}. {file.name}\")\n",
    "if len(full_files) > 3:\n",
    "    print(f\"  ... and {len(full_files) - 3} more files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Elevation Data with Optimization\n",
    "\n",
    "Extract data at elevation points from the combined CaSR files. We'll use time sampling to handle large datasets efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure extraction parameters\n",
    "time_sampling = 'all'  # Options: 'all', 'monthly', 'yearly', 'sample'\n",
    "max_records = 10000  # Maximum records per point to avoid memory issues\n",
    "file_types = ['temporal', 'full']  # Which file types to process\n",
    "\n",
    "print(f\"Extraction configuration:\")\n",
    "print(f\"  Time sampling: {time_sampling}\")\n",
    "print(f\"  Max records per point: {max_records}\")\n",
    "print(f\"  File types to process: {file_types}\")\n",
    "print(f\"\\nStarting elevation data extraction...\")\n",
    "\n",
    "# Process all files and extract elevation data\n",
    "extraction_results = elevation_extractor.process_all_files(\n",
    "    file_types=file_types,\n",
    "    time_sampling=time_sampling,\n",
    "    max_records=max_records\n",
    ")\n",
    "\n",
    "print(f\"\\nExtraction completed!\")\n",
    "print(f\"Processed {len(extraction_results)} file groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Extracted Data\n",
    "\n",
    "Save the extracted elevation data to files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results in multiple formats\n",
    "output_format = 'both'  # Options: 'csv', 'parquet', 'both'\n",
    "\n",
    "print(f\"Saving extracted data in {output_format} format...\")\n",
    "elevation_extractor.save_results(extraction_results, format=output_format)\n",
    "\n",
    "# Generate summary report\n",
    "elevation_extractor.generate_summary_report(extraction_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and Merge Elevation Data\n",
    "\n",
    "Now we'll filter and merge the elevation data to create a new dataset containing only non-null precipitation and SWE values. This step combines the functionality from `filter_merge_elevation_data.py` to create a clean, merged dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional libraries for filtering and merging\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "print(\"Additional libraries imported for filtering and merging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Elevation Data Filter\n",
    "\n",
    "Create a filter class to handle the merging and filtering of elevation data with non-null precipitation and SWE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElevationDataFilter:\n",
    "    \"\"\"Filter and merge elevation data based on non-null precipitation and SWE values.\"\"\"\n",
    "    \n",
    "    def __init__(self, elevation_dir, casr_dir, output_dir=None):\n",
    "        \"\"\"\n",
    "        Initialize the filter.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        elevation_dir : str\n",
    "            Path to directory containing elevation shapefiles\n",
    "        casr_dir : str\n",
    "            Path to directory containing CaSR NetCDF files\n",
    "        output_dir : str, optional\n",
    "            Output directory for filtered data\n",
    "        \"\"\"\n",
    "        self.elevation_dir = Path(elevation_dir)\n",
    "        self.casr_dir = Path(casr_dir)\n",
    "        self.output_dir = Path(output_dir) if output_dir else Path(\"data/output_data/filtered_elevation\")\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Store data\n",
    "        self.elevation_gdf = None\n",
    "        self.precipitation_data = None\n",
    "        self.swe_data = None\n",
    "        \n",
    "    def load_elevation_data(self, sample_size=None):\n",
    "        \"\"\"Load elevation shapefile data.\"\"\"\n",
    "        print(\"Loading elevation data...\")\n",
    "        \n",
    "        # Find shapefile in elevation directory\n",
    "        shp_files = list(self.elevation_dir.glob(\"*.shp\"))\n",
    "        if not shp_files:\n",
    "            raise FileNotFoundError(f\"No shapefile found in {self.elevation_dir}\")\n",
    "        \n",
    "        # Load the first shapefile found\n",
    "        shp_file = shp_files[0]\n",
    "        print(f\"Loading shapefile: {shp_file}\")\n",
    "        \n",
    "        try:\n",
    "            self.elevation_gdf = gpd.read_file(shp_file)\n",
    "            \n",
    "            # Sample data if requested\n",
    "            if sample_size and len(self.elevation_gdf) > sample_size:\n",
    "                print(f\"Sampling {sample_size} points from {len(self.elevation_gdf)} total points\")\n",
    "                self.elevation_gdf = self.elevation_gdf.sample(n=sample_size, random_state=42)\n",
    "            \n",
    "            print(f\"Loaded {len(self.elevation_gdf)} elevation points\")\n",
    "            print(f\"Elevation data columns: {list(self.elevation_gdf.columns)}\")\n",
    "            \n",
    "            # Identify elevation columns\n",
    "            self.elev_cols = [col for col in self.elevation_gdf.columns \n",
    "                             if 'elev' in col.lower() or col in ['min', 'max', 'mean', 'median']]\n",
    "            \n",
    "            if self.elev_cols:\n",
    "                print(f\"Elevation columns found: {self.elev_cols}\")\n",
    "                for col in self.elev_cols:\n",
    "                    if pd.api.types.is_numeric_dtype(self.elevation_gdf[col]):\n",
    "                        print(f\"{col} range: {self.elevation_gdf[col].min():.1f} - {self.elevation_gdf[col].max():.1f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading shapefile: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def find_casr_files(self):\n",
    "        \"\"\"Find precipitation and SWE files in the CaSR directory.\"\"\"\n",
    "        nc_files = list(self.casr_dir.glob(\"*.nc\"))\n",
    "        \n",
    "        precip_files = []\n",
    "        swe_files = []\n",
    "        \n",
    "        for f in nc_files:\n",
    "            if \"A_PR24_SFC\" in f.name:\n",
    "                precip_files.append(f)\n",
    "            elif \"P_SWE_LAND\" in f.name:\n",
    "                swe_files.append(f)\n",
    "        \n",
    "        print(f\"Found {len(precip_files)} precipitation files\")\n",
    "        print(f\"Found {len(swe_files)} SWE files\")\n",
    "        \n",
    "        return precip_files, swe_files\n",
    "\n",
    "print(\"ElevationDataFilter class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with data extraction and filtering methods\n",
    "def extract_data_at_points(self, nc_file, points_gdf, variable_name, sample_time=None):\n",
    "    \"\"\"Extract data from NetCDF file at elevation points.\"\"\"\n",
    "    print(f\"Extracting {variable_name} data from {nc_file.name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Open NetCDF file\n",
    "        ds = xr.open_dataset(nc_file)\n",
    "        \n",
    "        # Get data variable name\n",
    "        data_vars = [v for v in ds.data_vars if v != 'rotated_pole']\n",
    "        if not data_vars:\n",
    "            print(f\"No data variables found in {nc_file.name}\")\n",
    "            return None\n",
    "        \n",
    "        var_name = data_vars[0]  # Assume first variable is the main data\n",
    "        print(f\"Extracting variable: {var_name}\")\n",
    "        \n",
    "        # Sample time if requested\n",
    "        if sample_time and 'time' in ds.dims and ds.dims['time'] > sample_time:\n",
    "            print(f\"Sampling {sample_time} time steps from {ds.dims['time']} total\")\n",
    "            time_indices = np.linspace(0, ds.dims['time']-1, sample_time, dtype=int)\n",
    "            ds = ds.isel(time=time_indices)\n",
    "        \n",
    "        # Get coordinate information\n",
    "        if 'lon' in ds.coords and 'lat' in ds.coords:\n",
    "            lon_coord, lat_coord = 'lon', 'lat'\n",
    "        elif 'rlon' in ds.coords and 'rlat' in ds.coords:\n",
    "            lon_coord, lat_coord = 'rlon', 'rlat'\n",
    "        else:\n",
    "            print(f\"Could not identify coordinate variables\")\n",
    "            return None\n",
    "        \n",
    "        # Convert points to same CRS as NetCDF if needed\n",
    "        points_proj = points_gdf.copy()\n",
    "        if points_proj.crs and points_proj.crs.to_string() != 'EPSG:4326':\n",
    "            points_proj = points_proj.to_crs('EPSG:4326')\n",
    "        \n",
    "        # Extract data at each point\n",
    "        extracted_data = []\n",
    "        \n",
    "        for idx, row in points_proj.iterrows():\n",
    "            # Get point coordinates\n",
    "            geom = row.geometry\n",
    "            if hasattr(geom, 'x') and hasattr(geom, 'y'):\n",
    "                lon, lat = geom.x, geom.y\n",
    "            else:\n",
    "                centroid = geom.centroid\n",
    "                lon, lat = centroid.x, centroid.y\n",
    "            \n",
    "            try:\n",
    "                # Find nearest grid point\n",
    "                if 'lon' in ds.coords and 'lat' in ds.coords and ds.lon.ndim == 2:\n",
    "                    # Handle 2D coordinate arrays\n",
    "                    lon_2d = ds.lon.values\n",
    "                    lat_2d = ds.lat.values\n",
    "                    \n",
    "                    # Convert longitude if needed\n",
    "                    target_lon = lon if lon < 0 else lon - 360\n",
    "                    lon_2d_adj = np.where(lon_2d > 180, lon_2d - 360, lon_2d)\n",
    "                    \n",
    "                    # Find nearest point\n",
    "                    dist = np.sqrt((lon_2d_adj - target_lon)**2 + (lat_2d - lat)**2)\n",
    "                    min_idx = np.unravel_index(np.argmin(dist), dist.shape)\n",
    "                    rlat_idx, rlon_idx = min_idx\n",
    "                    \n",
    "                    point_data = ds.isel(rlat=rlat_idx, rlon=rlon_idx)\n",
    "                else:\n",
    "                    # Simple nearest neighbor selection\n",
    "                    point_data = ds.sel({lon_coord: lon, lat_coord: lat}, method='nearest')\n",
    "                \n",
    "                # Extract time series data\n",
    "                if 'time' in point_data[var_name].dims:\n",
    "                    times = point_data.time.values\n",
    "                    values = point_data[var_name].values\n",
    "                    \n",
    "                    for t, v in zip(times, values):\n",
    "                        data_dict = {\n",
    "                            'point_id': idx,\n",
    "                            'lon': lon,\n",
    "                            'lat': lat,\n",
    "                            'time': pd.to_datetime(t),\n",
    "                            variable_name: float(v) if not np.isnan(v) else np.nan\n",
    "                        }\n",
    "                        \n",
    "                        # Add elevation data\n",
    "                        for col in self.elev_cols:\n",
    "                            if col in row and pd.api.types.is_numeric_dtype(type(row[col])):\n",
    "                                data_dict[f'elevation_{col}'] = row[col]\n",
    "                        \n",
    "                        extracted_data.append(data_dict)\n",
    "                else:\n",
    "                    # Single value\n",
    "                    data_dict = {\n",
    "                        'point_id': idx,\n",
    "                        'lon': lon,\n",
    "                        'lat': lat,\n",
    "                        variable_name: float(point_data[var_name].values)\n",
    "                    }\n",
    "                    \n",
    "                    # Add elevation data\n",
    "                    for col in self.elev_cols:\n",
    "                        if col in row and pd.api.types.is_numeric_dtype(type(row[col])):\n",
    "                            data_dict[f'elevation_{col}'] = row[col]\n",
    "                    \n",
    "                    extracted_data.append(data_dict)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Could not extract data for point {idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        ds.close()\n",
    "        \n",
    "        if extracted_data:\n",
    "            df = pd.DataFrame(extracted_data)\n",
    "            print(f\"Extracted {len(df)} records for {variable_name}\")\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"No data extracted from {nc_file.name}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {nc_file.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Add the method to the class\n",
    "ElevationDataFilter.extract_data_at_points = extract_data_at_points\n",
    "print(\"Added extract_data_at_points method to ElevationDataFilter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add filtering and merging methods\n",
    "def filter_and_merge_data(self, precip_df, swe_df):\n",
    "    \"\"\"Filter and merge precipitation and SWE data for non-null values.\"\"\"\n",
    "    print(\"Filtering and merging data...\")\n",
    "    \n",
    "    # Merge on common keys\n",
    "    merge_keys = ['point_id', 'lon', 'lat']\n",
    "    if 'time' in precip_df.columns and 'time' in swe_df.columns:\n",
    "        merge_keys.append('time')\n",
    "    \n",
    "    # Merge dataframes\n",
    "    merged_df = pd.merge(\n",
    "        precip_df,\n",
    "        swe_df,\n",
    "        on=merge_keys,\n",
    "        suffixes=('_precip', '_swe'),\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Get elevation columns (handle duplicates from merge)\n",
    "    elev_cols_merged = [col for col in merged_df.columns if col.startswith('elevation_')]\n",
    "    \n",
    "    # Remove duplicate elevation columns\n",
    "    for col in elev_cols_merged:\n",
    "        if col.endswith('_swe') and col.replace('_swe', '_precip') in merged_df.columns:\n",
    "            # Keep only one version\n",
    "            merged_df[col.replace('_swe', '')] = merged_df[col]\n",
    "            merged_df = merged_df.drop([col, col.replace('_swe', '_precip')], axis=1)\n",
    "    \n",
    "    # Filter for non-null values\n",
    "    precip_col = [col for col in merged_df.columns if 'precipitation' in col.lower() or 'PR24' in col][0]\n",
    "    swe_col = [col for col in merged_df.columns if 'swe' in col.lower() or 'SWE' in col][0]\n",
    "    \n",
    "    print(f\"Total merged records: {len(merged_df)}\")\n",
    "    print(f\"Records with null precipitation: {merged_df[precip_col].isna().sum()}\")\n",
    "    print(f\"Records with null SWE: {merged_df[swe_col].isna().sum()}\")\n",
    "    \n",
    "    # Filter for non-null values in both variables\n",
    "    filtered_df = merged_df[\n",
    "        merged_df[precip_col].notna() & \n",
    "        merged_df[swe_col].notna()\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Records with non-null values in both variables: {len(filtered_df)}\")\n",
    "    \n",
    "    return filtered_df, precip_col, swe_col\n",
    "\n",
    "def analyze_elevation_patterns(self, filtered_df, precip_col, swe_col):\n",
    "    \"\"\"Analyze patterns in the filtered data by elevation.\"\"\"\n",
    "    print(\"Analyzing elevation patterns...\")\n",
    "    \n",
    "    # Find elevation columns\n",
    "    elev_cols = [col for col in filtered_df.columns if col.startswith('elevation_')]\n",
    "    \n",
    "    if not elev_cols:\n",
    "        print(\"No elevation columns found for analysis\")\n",
    "        return None\n",
    "    \n",
    "    # Use the first elevation column for analysis\n",
    "    elev_col = elev_cols[0]\n",
    "    \n",
    "    # Create elevation bins\n",
    "    filtered_df['elevation_bin'] = pd.cut(filtered_df[elev_col], bins=10)\n",
    "    \n",
    "    # Calculate statistics by elevation bin\n",
    "    stats_by_elevation = filtered_df.groupby('elevation_bin').agg({\n",
    "        precip_col: ['mean', 'std', 'count'],\n",
    "        swe_col: ['mean', 'std', 'count'],\n",
    "        'point_id': 'nunique'\n",
    "    }).round(2)\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    stats_by_elevation.columns = [\n",
    "        'precip_mean', 'precip_std', 'precip_count',\n",
    "        'swe_mean', 'swe_std', 'swe_count',\n",
    "        'unique_points'\n",
    "    ]\n",
    "    \n",
    "    # Calculate correlation between variables\n",
    "    if len(filtered_df) > 1:\n",
    "        correlation = filtered_df[[precip_col, swe_col]].corr().iloc[0, 1]\n",
    "        print(f\"Correlation between precipitation and SWE: {correlation:.3f}\")\n",
    "    \n",
    "    return stats_by_elevation\n",
    "\n",
    "# Add methods to the class\n",
    "ElevationDataFilter.filter_and_merge_data = filter_and_merge_data\n",
    "ElevationDataFilter.analyze_elevation_patterns = analyze_elevation_patterns\n",
    "print(\"Added filtering and analysis methods to ElevationDataFilter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add save and summary methods\n",
    "def save_results(self, filtered_df, stats_df, format='csv'):\n",
    "    \"\"\"Save filtered data and statistics.\"\"\"\n",
    "    print(f\"Saving results to {self.output_dir}\")\n",
    "    \n",
    "    # Save filtered data\n",
    "    if format in ['csv', 'both']:\n",
    "        csv_file = self.output_dir / \"filtered_elevation_data.csv\"\n",
    "        filtered_df.to_csv(csv_file, index=False)\n",
    "        print(f\"Saved filtered data to: {csv_file}\")\n",
    "        \n",
    "        if stats_df is not None:\n",
    "            stats_csv = self.output_dir / \"elevation_statistics.csv\"\n",
    "            stats_df.to_csv(stats_csv)\n",
    "            print(f\"Saved statistics to: {stats_csv}\")\n",
    "    \n",
    "    if format in ['parquet', 'both']:\n",
    "        parquet_file = self.output_dir / \"filtered_elevation_data.parquet\"\n",
    "        filtered_df.to_parquet(parquet_file, index=False)\n",
    "        print(f\"Saved filtered data to: {parquet_file}\")\n",
    "        \n",
    "        if stats_df is not None:\n",
    "            stats_parquet = self.output_dir / \"elevation_statistics.parquet\"\n",
    "            stats_df.to_parquet(stats_parquet)\n",
    "            print(f\"Saved statistics to: {stats_parquet}\")\n",
    "    \n",
    "    # Generate summary report\n",
    "    self.generate_summary_report(filtered_df, stats_df)\n",
    "\n",
    "def generate_summary_report(self, filtered_df, stats_df):\n",
    "    \"\"\"Generate a summary report of the filtering results.\"\"\"\n",
    "    summary = {\n",
    "        'processing_date': datetime.now().isoformat(),\n",
    "        'elevation_points_loaded': len(self.elevation_gdf) if self.elevation_gdf is not None else 0,\n",
    "        'filtered_records': len(filtered_df),\n",
    "        'unique_points_with_data': filtered_df['point_id'].nunique() if 'point_id' in filtered_df.columns else 0,\n",
    "        'time_range': None\n",
    "    }\n",
    "    \n",
    "    if 'time' in filtered_df.columns:\n",
    "        summary['time_range'] = {\n",
    "            'start': filtered_df['time'].min().isoformat() if pd.notna(filtered_df['time'].min()) else None,\n",
    "            'end': filtered_df['time'].max().isoformat() if pd.notna(filtered_df['time'].max()) else None\n",
    "        }\n",
    "    \n",
    "    # Save summary\n",
    "    summary_file = self.output_dir / \"filtering_summary.json\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"Summary saved to: {summary_file}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ELEVATION DATA FILTERING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Elevation points loaded: {summary['elevation_points_loaded']}\")\n",
    "    print(f\"Filtered records (non-null precip & SWE): {summary['filtered_records']}\")\n",
    "    print(f\"Unique points with valid data: {summary['unique_points_with_data']}\")\n",
    "    if summary['time_range']:\n",
    "        print(f\"Time range: {summary['time_range']['start']} to {summary['time_range']['end']}\")\n",
    "    \n",
    "    if stats_df is not None:\n",
    "        print(\"\\nElevation Statistics:\")\n",
    "        print(stats_df)\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Add methods to the class\n",
    "ElevationDataFilter.save_results = save_results\n",
    "ElevationDataFilter.generate_summary_report = generate_summary_report\n",
    "print(\"Added save and summary methods to ElevationDataFilter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and Configure the Filter\n",
    "\n",
    "Set up the elevation data filter with the appropriate directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filtered output directory\n",
    "filtered_output_dir = r\"data/output_data/filtered_elevation\"\n",
    "filtered_output_path = project_root / filtered_output_dir\n",
    "\n",
    "print(f\"Filtered output directory: {filtered_output_path}\")\n",
    "\n",
    "# Initialize the elevation data filter\n",
    "elevation_filter = ElevationDataFilter(\n",
    "    elevation_dir=str(elevation_path),\n",
    "    casr_dir=str(output_path),  # Use combined CaSR files\n",
    "    output_dir=str(filtered_output_path)\n",
    ")\n",
    "\n",
    "print(\"ElevationDataFilter initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Elevation Data for Filtering\n",
    "\n",
    "Load the elevation shapefile data with optional sampling for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load elevation data with sampling for testing\n",
    "sample_points = 100  # Adjust this number based on your needs\n",
    "\n",
    "elevation_filter.load_elevation_data(sample_size=sample_points)\n",
    "\n",
    "print(f\"\\nElevation data loaded with {len(elevation_filter.elevation_gdf)} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and Process CaSR Files\n",
    "\n",
    "Identify precipitation and SWE files from the combined CaSR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find precipitation and SWE files\n",
    "precip_files, swe_files = elevation_filter.find_casr_files()\n",
    "\n",
    "if not precip_files or not swe_files:\n",
    "    print(\"Missing precipitation or SWE files\")\n",
    "    print(\"Available files in CaSR directory:\")\n",
    "    for f in elevation_filter.casr_dir.glob(\"*.nc\"):\n",
    "        print(f\"  {f.name}\")\n",
    "else:\n",
    "    print(f\"\\nFound files for processing:\")\n",
    "    print(f\"Precipitation files: {[f.name for f in precip_files[:3]]}\")\n",
    "    print(f\"SWE files: {[f.name for f in swe_files[:3]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Data at Elevation Points\n",
    "\n",
    "Extract precipitation and SWE data at the elevation points from the combined NetCDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure extraction parameters\n",
    "sample_time = 10  # Number of time steps to sample for testing\n",
    "\n",
    "if precip_files and swe_files:\n",
    "    print(\"Processing sample files...\")\n",
    "    \n",
    "    # Extract precipitation data\n",
    "    precip_df = elevation_filter.extract_data_at_points(\n",
    "        precip_files[0], \n",
    "        elevation_filter.elevation_gdf, \n",
    "        'precipitation',\n",
    "        sample_time=sample_time\n",
    "    )\n",
    "    \n",
    "    # Extract SWE data\n",
    "    swe_df = elevation_filter.extract_data_at_points(\n",
    "        swe_files[0], \n",
    "        elevation_filter.elevation_gdf, \n",
    "        'swe',\n",
    "        sample_time=sample_time\n",
    "    )\n",
    "    \n",
    "    if precip_df is not None and swe_df is not None:\n",
    "        print(f\"\\nExtracted data successfully:\")\n",
    "        print(f\"Precipitation records: {len(precip_df)}\")\n",
    "        print(f\"SWE records: {len(swe_df)}\")\n",
    "    else:\n",
    "        print(\"Failed to extract data from files\")\n",
    "else:\n",
    "    print(\"No precipitation or SWE files found for processing\")\n",
    "    precip_df = None\n",
    "    swe_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Merge Data\n",
    "\n",
    "Now filter and merge the precipitation and SWE data to keep only records with non-null values in both variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if precip_df is not None and swe_df is not None:\n",
    "    # Filter and merge data\n",
    "    filtered_df, precip_col, swe_col = elevation_filter.filter_and_merge_data(precip_df, swe_df)\n",
    "    \n",
    "    print(f\"\\nFiltered and merged data:\")\n",
    "    print(f\"Total records with non-null values: {len(filtered_df)}\")\n",
    "    print(f\"Precipitation column: {precip_col}\")\n",
    "    print(f\"SWE column: {swe_col}\")\n",
    "    \n",
    "    # Show sample of filtered data\n",
    "    if len(filtered_df) > 0:\n",
    "        print(f\"\\nSample of filtered data:\")\n",
    "        print(filtered_df.head())\n",
    "        \n",
    "        # Show data types\n",
    "        print(f\"\\nData types:\")\n",
    "        print(filtered_df.dtypes)\n",
    "    else:\n",
    "        print(\"No records found with non-null values in both variables\")\n",
    "        filtered_df = None\n",
    "else:\n",
    "    print(\"Cannot proceed with filtering - missing precipitation or SWE data\")\n",
    "    filtered_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Elevation Patterns\n",
    "\n",
    "Analyze the filtered data to understand patterns by elevation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filtered_df is not None and len(filtered_df) > 0:\n",
    "    # Analyze elevation patterns\n",
    "    stats_df = elevation_filter.analyze_elevation_patterns(filtered_df, precip_col, swe_col)\n",
    "    \n",
    "    if stats_df is not None:\n",
    "        print(f\"\\nElevation pattern analysis completed:\")\n",
    "        print(stats_df)\n",
    "        \n",
    "        # Show correlation analysis\n",
    "        if len(filtered_df) > 1:\n",
    "            correlation_matrix = filtered_df[[precip_col, swe_col]].corr()\n",
    "            print(f\"\\nCorrelation Matrix:\")\n",
    "            print(correlation_matrix)\n",
    "    else:\n",
    "        print(\"Could not perform elevation pattern analysis\")\n",
    "        stats_df = None\n",
    "else:\n",
    "    print(\"No filtered data available for elevation pattern analysis\")\n",
    "    stats_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Filtered and Merged Data\n",
    "\n",
    "Save the filtered and merged dataset to files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filtered_df is not None and len(filtered_df) > 0:\n",
    "    # Save results in multiple formats\n",
    "    save_format = 'both'  # Options: 'csv', 'parquet', 'both'\n",
    "    \n",
    "    print(f\"Saving filtered and merged data in {save_format} format...\")\n",
    "    elevation_filter.save_results(filtered_df, stats_df, format=save_format)\n",
    "    \n",
    "    print(f\"\\nFiltering and merging process completed successfully!\")\n",
    "    print(f\"Output files saved to: {elevation_filter.output_dir}\")\n",
    "    \n",
    "    # List output files\n",
    "    output_files = list(elevation_filter.output_dir.glob('*'))\n",
    "    print(f\"\\nGenerated files:\")\n",
    "    for i, file_path in enumerate(output_files, 1):\n",
    "        if file_path.is_file():\n",
    "            file_size = file_path.stat().st_size / 1024  # Size in KB\n",
    "            print(f\"  {i}. {file_path.name} ({file_size:.1f} KB)\")\n",
    "else:\n",
    "    print(\"No data to save - filtering process did not produce valid results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated the complete workflow for:\n",
    "\n",
    "1. **CaSR File Combination**: Combined NetCDF files across temporal and spatial dimensions\n",
    "2. **Elevation Data Extraction**: Extracted precipitation and SWE data at specific elevation points\n",
    "3. **Data Filtering and Merging**: Created a clean dataset with only non-null precipitation and SWE values\n",
    "4. **Elevation Pattern Analysis**: Analyzed relationships between elevation and climate variables\n",
    "5. **Data Export**: Saved the filtered and merged dataset for further analysis\n",
    "\n",
    "The final output includes:\n",
    "- **filtered_elevation_data.csv/parquet**: Main dataset with non-null precipitation and SWE values\n",
    "- **elevation_statistics.csv/parquet**: Statistical analysis by elevation bins\n",
    "- **filtering_summary.json**: Summary report of the filtering process\n",
    "\n",
    "This workflow provides a foundation for analyzing snow drought patterns across different elevation zones using the CaSR dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
